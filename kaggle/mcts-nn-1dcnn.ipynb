{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70089,"databundleVersionId":9515283,"sourceType":"competition"},{"sourceId":10020397,"sourceType":"datasetVersion","datasetId":5909723},{"sourceId":10020545,"sourceType":"datasetVersion","datasetId":6170268}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -qq /kaggle/input/wheels/lightning-2.4.0-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T14:47:12.528964Z","iopub.execute_input":"2024-11-26T14:47:12.529415Z","iopub.status.idle":"2024-11-26T14:47:58.193015Z","shell.execute_reply.started":"2024-11-26T14:47:12.529347Z","shell.execute_reply":"2024-11-26T14:47:58.191501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom typing import Optional, List\nimport polars\nimport os\n\nimport torch \nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nimport lightning.pytorch as pl\nfrom lightning.pytorch.callbacks import EarlyStopping\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"PyTorch Lightning version: {pl.__version__}\")\n\nimport sys\nsys.path.append(\"/kaggle/input/mcts-artifacts\")\nfrom preproc import process_test_data\nimport kaggle_evaluation.mcts_inference_server","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-26T14:53:05.608133Z","iopub.execute_input":"2024-11-26T14:53:05.60855Z","iopub.status.idle":"2024-11-26T14:53:05.617021Z","shell.execute_reply.started":"2024-11-26T14:53:05.608513Z","shell.execute_reply":"2024-11-26T14:53:05.615757Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n### load artifacts\n","metadata":{}},{"cell_type":"code","source":"# Specify the path where you want to save the serialized function\nnn_1dcnn_artifacts_path = '/kaggle/input/mcts-artifacts/nn-1dcnn_predict.pt'\n\n# Load the function from the file\nnn_1dcnn_artifacts = torch.load(nn_1dcnn_artifacts_path, weights_only=False)\n\nlen(nn_1dcnn_artifacts['models'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T14:56:26.674694Z","iopub.execute_input":"2024-11-26T14:56:26.675106Z","iopub.status.idle":"2024-11-26T14:56:26.791949Z","shell.execute_reply.started":"2024-11-26T14:56:26.675073Z","shell.execute_reply":"2024-11-26T14:56:26.790771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SoftOrdering1DCNN(pl.LightningModule):\n\n    def __init__(self, \n            num_input_dim: int,\n            cat_input_dims: list[int],\n            output_dim: int,\n            sign_size: int = 32,\n            cha_input: int = 16, \n            cha_hidden: int = 32,\n            K: int = 2,\n            dropout_input: float = 0.2,\n            dropout_hidden: float = 0.2, \n            dropout_output: float = 0.2,\n            embedding_dropout: float = 0.2,\n            learning_rate: float = 1e-3,\n            weight_decay: float = 1e-5,\n            embedding_dim: Optional[List[int]] = None,\n            pct_start: float = 0.2,\n            div_factor: float = 10.0,\n            final_div_factor: float = 1e4):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Initialize embedding dimensions if not provided\n        if embedding_dim is None:\n            embedding_dim = [min(50, int(1 + np.ceil(np.sqrt(dim)))) for dim in cat_input_dims]\n        elif len(embedding_dim) != len(cat_input_dims):\n            raise ValueError(\"Length of embedding_dim must match number of categorical features.\")\n        \n        self.embedding_dim = embedding_dim\n        self.embedding_dropout = embedding_dropout\n        \n        # Create embedding layers\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(dim, emb_dim) for dim, emb_dim in zip(cat_input_dims, embedding_dim)]\n        )\n        self.embedding_dropout_layer = nn.Dropout(self.embedding_dropout)\n\n        # Calculate total input dimension after embeddings\n        total_embedding_dim = sum(self.embedding_dim)\n        total_input_dim = num_input_dim + total_embedding_dim\n\n        # CNN architecture parameters\n        hidden_size = sign_size * cha_input\n        self.sign_size1 = sign_size\n        self.sign_size2 = sign_size//2\n        self.output_size = (sign_size//4) * cha_hidden\n        self.cha_input = cha_input\n        self.cha_hidden = cha_hidden\n        self.K = K\n\n        # Input projection\n        self.batch_norm1 = nn.BatchNorm1d(total_input_dim)\n        self.dropout1 = nn.Dropout(dropout_input)\n        dense1 = nn.Linear(total_input_dim, hidden_size, bias=False)\n        self.dense1 = nn.utils.weight_norm(dense1)\n\n        # 1st conv layer\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n        conv1 = nn.Conv1d(\n            cha_input, \n            cha_input*K, \n            kernel_size=5, \n            stride=1, \n            padding=2,  \n            groups=cha_input, \n            bias=False)\n        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size=self.sign_size2)\n\n        # 2nd conv layer\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n        self.dropout_c2 = nn.Dropout(dropout_hidden)\n        conv2 = nn.Conv1d(\n            cha_input*K, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n\n        # 3rd conv layer\n        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n        self.dropout_c3 = nn.Dropout(dropout_hidden)\n        conv3 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=3, \n            stride=1, \n            padding=1, \n            bias=False)\n        self.conv3 = nn.utils.weight_norm(conv3, dim=None)\n\n        # 4th conv layer\n        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n        conv4 = nn.Conv1d(\n            cha_hidden, \n            cha_hidden, \n            kernel_size=5, \n            stride=1, \n            padding=2, \n            groups=cha_hidden, \n            bias=False)\n        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n\n        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n        self.flt = nn.Flatten()\n\n        # Output head\n        self.batch_norm2 = nn.BatchNorm1d(self.output_size)\n        self.dropout2 = nn.Dropout(dropout_output)\n        dense2 = nn.Linear(self.output_size, output_dim, bias=False)\n        self.dense2 = nn.utils.weight_norm(dense2)\n\n        # Training parameters\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.pct_start = pct_start\n        self.div_factor = div_factor\n        self.final_div_factor = final_div_factor\n\n        # Initialize lists to store validation outputs\n        self.validation_targets = []\n        self.validation_predictions = []\n\n    def forward(self, x_num, x_cat):\n        # Process categorical variables\n        embedded = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n        embedded = torch.cat(embedded, dim=1)\n        embedded = self.embedding_dropout_layer(embedded)\n        \n        # Concatenate numerical and embedded categorical features\n        x = torch.cat([x_num, embedded], dim=1)\n\n        # Input projection\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = nn.functional.celu(self.dense1(x))\n\n        # Reshape for CNN\n        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n\n        # CNN backbone\n        x = self.batch_norm_c1(x)\n        x = nn.functional.leaky_relu(self.conv1(x))\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = nn.functional.leaky_relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c3(x)\n        x = self.dropout_c3(x)\n        x = nn.functional.leaky_relu(self.conv3(x))\n\n        x = self.batch_norm_c4(x)\n        x = self.conv4(x)\n        x = x + x_s\n        x = nn.functional.leaky_relu(x)\n\n        x = self.avg_po_c4(x)\n        x = self.flt(x)\n\n        # Output head\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n        x = nn.functional.hardtanh(x)\n\n        return x.squeeze(-1)\n\n    def training_step(self, batch, batch_idx):\n        x_num, x_cat, y = batch\n        y_hat = self(x_num, x_cat)\n        loss = F.mse_loss(y_hat, y)\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x_num, x_cat, y = batch\n        y_hat = self(x_num, x_cat)\n        loss = F.mse_loss(y_hat, y)\n        self.log('valid_loss', loss, prog_bar=True)\n        # Store targets and predictions for later use\n        self.validation_targets.append(y)\n        self.validation_predictions.append(y_hat)\n        return loss\n    \n    def predict_step(self, batch, batch_idx):\n        if len(batch) == 2:\n            x_num, x_cat = batch\n        elif len(batch) == 3:\n            x_num, x_cat, _ = batch\n        y_hat = self(x_num, x_cat)\n        return y_hat\n\n    def on_validation_epoch_end(self):\n        # Concatenate all targets and predictions\n        y = torch.cat(self.validation_targets)\n        y_hat = torch.cat(self.validation_predictions)\n        rmse = torch.sqrt(F.mse_loss(y_hat, y))\n        self.log('val_rmse', rmse, prog_bar=True)\n        # Clear the lists for next epoch\n        self.validation_targets.clear()\n        self.validation_predictions.clear()\n                \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.parameters(), \n            lr=self.learning_rate, \n            weight_decay=self.weight_decay,\n        )\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=self.learning_rate,\n            total_steps=self.trainer.estimated_stepping_batches,\n            pct_start=self.pct_start,\n            div_factor=self.div_factor,\n            final_div_factor=self.final_div_factor,\n            anneal_strategy='cos',\n            cycle_momentum=True,\n            base_momentum=0.85,\n            max_momentum=0.95,\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\",\n            },\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T14:56:29.444588Z","iopub.execute_input":"2024-11-26T14:56:29.445026Z","iopub.status.idle":"2024-11-26T14:56:29.472854Z","shell.execute_reply.started":"2024-11-26T14:56:29.444991Z","shell.execute_reply":"2024-11-26T14:56:29.47173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SoftOrdering1DCNNInference:\n    def __init__(\n        self,\n        models_state_dicts,\n        models_hparams,\n        numerical_cols,\n        categorical_cols,\n        encoder,\n        scaler,\n        lgbm_encoders,\n    ):\n        \"\"\"Initialize inference class with trained artifacts\n        \n        Args:\n            models_state_dicts: List of model state dictionaries\n            models_hparams: List of model hyperparameters\n            numerical_cols: List of numerical column names\n            categorical_cols: List of categorical column names\n            encoder: Fitted OrdinalEncoder for categorical features\n            scaler: Fitted StandardScaler for numerical features\n            lgbm_encoders: List of LightGBM encoders for feature engineering\n        \"\"\"\n        self.numerical_cols = numerical_cols\n        self.categorical_cols = categorical_cols\n        self.encoder = encoder\n        self.scaler = scaler\n        self.lgbm_encoders = lgbm_encoders\n\n        # Load models\n        self.models = []\n        for state_dict, hparams in zip(models_state_dicts, models_hparams):\n            model = SoftOrdering1DCNN(**hparams)\n            model.load_state_dict(state_dict)\n            model.eval()  # Set to evaluation mode\n            self.models.append(model)\n\n        print(\"len(numerical_cols):\", len(numerical_cols))\n        print(\"len(categorical_cols):\", len(categorical_cols))\n\n    def predict_array(self, df_test):\n        \"\"\"Make predictions on test data\n        \n        Args:\n            df_test: pandas DataFrame containing test features\n            \n        Returns:\n            numpy array of predictions\n        \"\"\"\n        # Preprocess test data\n        test_processed = process_test_data(\n            df_test,\n            self.numerical_cols,\n            self.categorical_cols,\n            self.encoder,\n            self.scaler,\n            include_position_features=False,\n            include_text_features=False,\n        )\n\n        # Initialize predictions array\n        predictions = np.zeros(len(df_test))\n\n        # Get predictions from all models\n        for lgbm_encoder, model in zip(self.lgbm_encoders, self.models):\n            # Prepare numerical and categorical features\n            X_test_num = test_processed[self.numerical_cols].copy()\n            X_test_cat = test_processed[self.categorical_cols].copy()\n\n            # Add LGBM encoder leaves features\n            lgbm_features = lgbm_encoder.transform(\n                test_processed[self.numerical_cols + self.categorical_cols]\n            )\n            X_test_cat = pd.concat([X_test_cat, lgbm_features], axis=1)\n            _categorical_cols = self.categorical_cols + lgbm_encoder.new_columns\n\n            # Convert to tensors and get predictions\n            with torch.no_grad():\n                X_test_num_tensor = torch.tensor(\n                    X_test_num[self.numerical_cols].values, dtype=torch.float32\n                )\n                X_test_cat_tensor = torch.tensor(\n                    X_test_cat[_categorical_cols].values, dtype=torch.int32\n                )\n                batch_predictions = model(\n                    X_test_num_tensor, X_test_cat_tensor\n                ).cpu().numpy().flatten()\n\n            predictions += batch_predictions\n\n        # Average predictions across models\n        predictions /= len(self.models)\n        return predictions\n\n    def predict(self, test: polars.DataFrame, sample_sub: polars.DataFrame):\n        test_pd = test.to_pandas()\n        predictions = self.predict_array(test_pd)\n        submission = sample_sub.with_columns(polars.Series(\"utility_agent1\", predictions))\n        return submission\n\n\n# Create inference class\nmodel_1dcnn = SoftOrdering1DCNNInference(\n    models_state_dicts=nn_1dcnn_artifacts['models'],\n    models_hparams=nn_1dcnn_artifacts['models_hparams'],\n    numerical_cols=nn_1dcnn_artifacts['numerical_cols'],\n    categorical_cols=nn_1dcnn_artifacts['categorical_cols'],\n    encoder=nn_1dcnn_artifacts['encoder'],\n    scaler=nn_1dcnn_artifacts['scaler'],\n    lgbm_encoders=nn_1dcnn_artifacts['lgbm_encoders'],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T14:56:55.977786Z","iopub.execute_input":"2024-11-26T14:56:55.978198Z","iopub.status.idle":"2024-11-26T14:56:56.226237Z","shell.execute_reply.started":"2024-11-26T14:56:55.978164Z","shell.execute_reply":"2024-11-26T14:56:56.224909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# sanity check\ntest = polars.read_csv(\"/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv\")\nsample_sub = polars.read_csv(\"/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv\")\nmodel_1dcnn.predict(test, sample_sub)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***\n### inference","metadata":{}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(model_1dcnn.predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/test.csv',\n            '/kaggle/input/um-game-playing-strength-of-mcts-variants/sample_submission.csv'\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-11-26T15:04:32.352389Z","iopub.execute_input":"2024-11-26T15:04:32.352839Z","iopub.status.idle":"2024-11-26T15:04:33.0558Z","shell.execute_reply.started":"2024-11-26T15:04:32.352804Z","shell.execute_reply":"2024-11-26T15:04:33.05474Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***","metadata":{}}]}