{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Callable\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from preproc import process_train_data, process_test_data\n",
    "from transformer import LGBMLeavesEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular.models.common.layers import GatedFeatureLearningUnit\n",
    "from pytorch_tabular.models.common.layers.activations import t_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds for reproducibility\n",
    "np.random.seed(2112)\n",
    "pl.seed_everything(2112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful callbacks\n",
    "class LearningRateMonitor(pl.Callback):\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx % 100 == 0:  # Log every 100 batches\n",
    "            lr = pl_module.optimizers().param_groups[0]['lr']\n",
    "            pl_module.log('learning_rate', lr, prog_bar=True)   \n",
    "\n",
    "class BestValRMSELogger(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_rmse = float('inf')\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        current_val_rmse = trainer.callback_metrics.get('val_rmse')\n",
    "        if current_val_rmse is not None:\n",
    "            self.best_val_rmse = min(self.best_val_rmse, current_val_rmse)\n",
    "            pl_module.log('best_val_rmse', self.best_val_rmse, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some paths\n",
    "path_raw = Path(\"../data/raw\")\n",
    "path_processed = Path(\"../data/processed\")\n",
    "path_results = Path(\"../data/results\")\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_csv(path_raw / \"train.csv\")\n",
    "df_test = pd.read_csv(path_raw / \"test.csv\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the feature selection results\n",
    "fs_type = 'uni95'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mapping = {\n",
    "    \"full\": None,\n",
    "    \"fsv2\": '../feat_selection/select_optuna_lgbm_v2.json',\n",
    "    \"fsv3\": '../feat_selection/select_optuna_catb.json',\n",
    "    \"fsv4\": '../feat_selection/select_optuna_lgbm_v3.json',\n",
    "    \"fsv23\": '../feat_selection/select_optuna_combined_v23.json',\n",
    "    \"fsv24\": '../feat_selection/select_optuna_combined_v24.json',\n",
    "    \"fsv34\": '../feat_selection/select_optuna_combined_v34.json',\n",
    "    \"int95\": '../feat_selection/feat_selection_intersection_at_95.json',\n",
    "    \"int96\": '../feat_selection/feat_selection_intersection_at_96.json',\n",
    "    \"int97\": '../feat_selection/feat_selection_intersection_at_97.json',\n",
    "    \"int98\": '../feat_selection/feat_selection_intersection_at_98.json',\n",
    "    \"int99\": '../feat_selection/feat_selection_intersection_at_99.json',\n",
    "    \"uni80\": '../feat_selection/feat_selection_union_at_80.json',\n",
    "    \"uni85\": '../feat_selection/feat_selection_union_at_85.json',\n",
    "    \"uni90\": '../feat_selection/feat_selection_union_at_90.json',\n",
    "    \"uni95\": '../feat_selection/feat_selection_union_at_95.json',\n",
    "}\n",
    "\n",
    "fs_path = fs_mapping[fs_type]\n",
    "\n",
    "if fs_path is None:\n",
    "    feature_selection = dict()\n",
    "else:\n",
    "    with open(fs_path, 'r') as f:\n",
    "        feature_selection = json.load(f)\n",
    "\n",
    "# Extract the selected features\n",
    "numerical_cols = feature_selection.get('numerical', None)\n",
    "categorical_cols = feature_selection.get('categorical', None)\n",
    "\n",
    "# text_cols = [\"LudRules\",]\n",
    "text_cols = list()\n",
    "\n",
    "print(\"Numerical features:\", len(numerical_cols) if numerical_cols else 0)\n",
    "print(\"Categorical features:\", len(categorical_cols) if categorical_cols else 0)\n",
    "print(\"Text features:\", len(text_cols) if text_cols else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, numerical_cols, categorical_cols, encoder, scaler = process_train_data(\n",
    "    df_train,\n",
    "    scale=True,\n",
    "    scale_type='minmax',\n",
    "    numerical_cols=numerical_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    include_position_features=False,\n",
    "    include_text_features=False,\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Numerical Columns:\", len(numerical_cols))\n",
    "print(\"Categorical Columns:\", len(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['utility_agent1'], bins=100)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Utility Agent 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_input_dims = df_train[categorical_cols].nunique(axis=0).values.tolist()\n",
    "print(cat_input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = pickle.load(open('../data/splits/cv1_Game.pkl', 'rb'))\n",
    "split2 = pickle.load(open('../data/splits/cv2_Game.pkl', 'rb'))\n",
    "split3 = pickle.load(open('../data/splits/cv3_Game.pkl', 'rb'))\n",
    "\n",
    "# split1 = pickle.load(open('../data/splits/cv1_GameRulesetName.pkl', 'rb'))\n",
    "# split2 = pickle.load(open('../data/splits/cv2_GameRulesetName.pkl', 'rb'))\n",
    "# split3 = pickle.load(open('../data/splits/cv3_GameRulesetName.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDALF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "            num_input_dim: int,\n",
    "            cat_input_dims: list[int],\n",
    "            output_dim: int,\n",
    "            dropout: float,\n",
    "            embedding_dropout: float,\n",
    "            learning_rate: float = 1e-3,\n",
    "            weight_decay: float = 1e-5,\n",
    "            initialization: str = 'kaiming_uniform',\n",
    "            embedding_dim: Optional[List[int]] = None,\n",
    "            pct_start: float = 0.2,\n",
    "            div_factor: float = 10.0,\n",
    "            final_div_factor: float = 1e4,\n",
    "            n_stages: int = 6,\n",
    "            feature_mask_function: Callable = t_softmax,\n",
    "            feature_sparsity: float = 0.3,\n",
    "            learnable_sparsity: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.dropout = dropout\n",
    "        self.embedding_dropout = embedding_dropout\n",
    "        self.pct_start = pct_start\n",
    "        self.div_factor = div_factor\n",
    "        self.final_div_factor = final_div_factor\n",
    "\n",
    "        # Initialize embedding dimensions if not provided\n",
    "        if embedding_dim is None:\n",
    "            # Rule of thumb: min(50, num_unique // 2 + 1) for each categorical feature\n",
    "            embedding_dim = [min(50, int(1 + np.ceil(np.sqrt(dim)))) for dim in cat_input_dims]\n",
    "\n",
    "        elif len(embedding_dim) != len(cat_input_dims):\n",
    "            raise ValueError(\"Length of embedding_dim must match number of categorical features.\")\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create embedding layers\n",
    "        self.create_embeddings(cat_input_dims, embedding_dim)\n",
    "\n",
    "        # Create backbone layers\n",
    "        self.create_backbone(\n",
    "            num_input_dim,\n",
    "            n_stages,\n",
    "            feature_mask_function,\n",
    "            dropout,\n",
    "            feature_sparsity,\n",
    "            learnable_sparsity\n",
    "        )\n",
    "\n",
    "        # Create head layers\n",
    "        self.create_head(output_dim)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.initialization = initialization\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # Initialize lists to store validation outputs\n",
    "        self.validation_targets = []\n",
    "        self.validation_predictions = []\n",
    "\n",
    "    def create_embeddings(self, cat_input_dims: list[int], embedding_dim: list[int]):\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(dim, emb_dim) for dim, emb_dim in zip(cat_input_dims, embedding_dim)]\n",
    "        )\n",
    "        self.embedding_dropout_layer = nn.Dropout(self.embedding_dropout)\n",
    "\n",
    "    def create_backbone(\n",
    "            self, \n",
    "            num_input_dim: int,\n",
    "            n_stages: int,\n",
    "            feature_mask_function: Callable,\n",
    "            dropout: float,\n",
    "            feature_sparsity: float,\n",
    "            learnable_sparsity: bool\n",
    "        ):\n",
    "        # Calculate total input dimension after embeddings\n",
    "        total_embedding_dim = sum(self.embedding_dim)\n",
    "        total_input_dim = num_input_dim + total_embedding_dim\n",
    "\n",
    "        self.backbone = GatedFeatureLearningUnit(\n",
    "            n_features_in=total_input_dim,\n",
    "            n_stages=n_stages,\n",
    "            feature_mask_function=feature_mask_function,\n",
    "            dropout=dropout,\n",
    "            feature_sparsity=feature_sparsity,\n",
    "            learnable_sparsity=learnable_sparsity,\n",
    "        )\n",
    "        self.backbone_output_size = total_input_dim\n",
    "\n",
    "    def create_head(self, output_dim: int):\n",
    "        # Output layer\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.backbone_output_size),\n",
    "            nn.Linear(self.backbone_output_size, output_dim)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if any(module is m for m in self.head.modules()):\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "                else:\n",
    "                    if self.initialization == 'kaiming_uniform':\n",
    "                        nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "                    elif self.initialization == 'kaiming_normal':\n",
    "                        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                    elif self.initialization == 'xavier_uniform':\n",
    "                        nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                    elif self.initialization == 'xavier_normal':\n",
    "                        nn.init.xavier_normal_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported initialization method: {self.initialization}\")\n",
    "                \n",
    "                # Initialize bias to small values\n",
    "                if module.bias is not None:\n",
    "                    nn.init.uniform_(module.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Process categorical variables\n",
    "        embedded = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        embedded = self.embedding_dropout_layer(embedded)\n",
    "        \n",
    "        # Concatenate numerical and embedded categorical features\n",
    "        x = torch.cat([x_num, embedded], dim=1)\n",
    "        \n",
    "        # Pass through backbone\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Pass through head\n",
    "        x = self.head(x)\n",
    "        x = nn.functional.hardtanh(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        # Store targets and predictions for later use\n",
    "        self.validation_targets.append(y)\n",
    "        self.validation_predictions.append(y_hat)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        if len(batch) == 2:\n",
    "            x_num, x_cat = batch\n",
    "        elif len(batch) == 3:\n",
    "            x_num, x_cat, _ = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        return y_hat\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all targets and predictions\n",
    "        y = torch.cat(self.validation_targets)\n",
    "        y_hat = torch.cat(self.validation_predictions)\n",
    "        rmse = torch.sqrt(F.mse_loss(y_hat, y))\n",
    "        self.log('val_rmse', rmse, prog_bar=True)\n",
    "        # Clear the lists for next epoch\n",
    "        self.validation_targets.clear()\n",
    "        self.validation_predictions.clear()\n",
    "                \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=self.pct_start,\n",
    "            div_factor=self.div_factor,\n",
    "            final_div_factor=self.final_div_factor,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=True,\n",
    "            base_momentum=0.85,\n",
    "            max_momentum=0.95,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 512,\n",
    "    'embedding_dropout': 0.2,\n",
    "    \n",
    "    # GANDALF config\n",
    "    'dropout': 0.0,\n",
    "    'feature_sparsity': 0.8,\n",
    "    'learnable_sparsity': True,\n",
    "    'n_stages': 1,\n",
    "\n",
    "    'learning_rate': 5e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'pct_start': 0.05,\n",
    "    'div_factor': 100.0,\n",
    "    'final_div_factor': 1.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(\n",
    "        df_train,\n",
    "        numerical_cols,\n",
    "        categorical_cols,\n",
    "        target='utility_agent1',\n",
    "        split_list=None,\n",
    "    ):\n",
    "    \"\"\"Train multiple GANDALF models using cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        df_train: Training dataframe\n",
    "        numerical_cols: List of numerical column names\n",
    "        categorical_cols: List of categorical column names \n",
    "        target: Target column name\n",
    "        split_list: List of train/val splits for cross-validation\n",
    "    \n",
    "    Returns:\n",
    "        trained_models: List of trained GANDALF models\n",
    "        lgbm_encoders: List of fitted LGBM encoders\n",
    "        oof: Out-of-fold predictions dataframe\n",
    "        oof_scores: List of RMSE scores for each fold\n",
    "    \"\"\"\n",
    "    trained_models = list()\n",
    "    lgbm_encoders = list()\n",
    "    oof = pd.DataFrame(index=df_train.index, columns=[f'{target}_true', f'{target}_pred'])\n",
    "    oof_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for fold, (train_index, val_index) in enumerate(split_list, 1):\n",
    "        print(f\"Fold {fold}\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_train = df_train.iloc[train_index][numerical_cols + categorical_cols]\n",
    "        y_train = df_train.iloc[train_index][target]\n",
    "        X_valid = df_train.iloc[val_index][numerical_cols + categorical_cols]\n",
    "        y_valid = df_train.iloc[val_index][target]\n",
    "\n",
    "        lgbm_encoder = LGBMLeavesEncoder(\n",
    "            num_cols=numerical_cols,\n",
    "            cat_cols=categorical_cols,\n",
    "            task='regression',\n",
    "            n_estimators=100,\n",
    "            num_leaves=127,\n",
    "            random_state=2112,\n",
    "            verbosity=-1,\n",
    "        )\n",
    "        lgbm_encoder.fit(X_train, y_train)\n",
    "        X_train_leaves_lgbm = lgbm_encoder.transform(X_train, verbose=True)\n",
    "        X_valid_leaves_lgbm = lgbm_encoder.transform(X_valid)\n",
    "        \n",
    "        X_train = pd.concat([X_train, X_train_leaves_lgbm], axis=1)\n",
    "        X_valid = pd.concat([X_valid, X_valid_leaves_lgbm], axis=1) \n",
    "\n",
    "        _categorical_cols = (\n",
    "            categorical_cols + lgbm_encoder.new_columns\n",
    "        )\n",
    "        _cat_input_dims = (\n",
    "            df_train[categorical_cols].nunique(axis=0).values.tolist() + [127]*len(lgbm_encoder.new_columns)\n",
    "        )\n",
    "\n",
    "        lgbm_encoders.append(lgbm_encoder)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(X_train[numerical_cols].values, dtype=torch.float32),\n",
    "            torch.tensor(X_train[_categorical_cols].values, dtype=torch.int32),\n",
    "            torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        valid_dataset = TensorDataset(\n",
    "            torch.tensor(X_valid[numerical_cols].values, dtype=torch.float32),\n",
    "            torch.tensor(X_valid[_categorical_cols].values, dtype=torch.int32),\n",
    "            torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        model = GANDALF(\n",
    "            num_input_dim=len(numerical_cols),\n",
    "            cat_input_dims=_cat_input_dims,\n",
    "            output_dim=1,\n",
    "            embedding_dropout=params['embedding_dropout'],\n",
    "            learning_rate=params['learning_rate'], \n",
    "            weight_decay=params['weight_decay'],\n",
    "            initialization=\"kaiming_uniform\",\n",
    "            pct_start=params[\"pct_start\"],\n",
    "            div_factor=params[\"div_factor\"],\n",
    "            final_div_factor=params[\"final_div_factor\"],\n",
    "            # GANDALF config\n",
    "            dropout=params['dropout'],\n",
    "            n_stages=params['n_stages'],\n",
    "            feature_sparsity=params['feature_sparsity'],\n",
    "            learnable_sparsity=params['learnable_sparsity'],\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=100,\n",
    "            accelerator=\"mps\", \n",
    "            callbacks=[\n",
    "                EarlyStopping(\n",
    "                    monitor='val_rmse',\n",
    "                    patience=10,\n",
    "                    mode='min',\n",
    "                    verbose=False\n",
    "                ), \n",
    "                LearningRateMonitor(), \n",
    "                BestValRMSELogger(),\n",
    "                ModelCheckpoint(monitor='val_rmse', mode='min', save_top_k=1),\n",
    "            ],\n",
    "        )\n",
    "        trainer.fit(\n",
    "            model, \n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "        )\n",
    "\n",
    "        # Load the best model\n",
    "        best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "        model = GANDALF.load_from_checkpoint(best_model_path)\n",
    "        trained_models.append(model)\n",
    "\n",
    "        # Predict on validation set using trainer.predict with the prediction DataLoader\n",
    "        predictions = trainer.predict(model, dataloaders=valid_loader)\n",
    "        y_pred = torch.cat(predictions).squeeze().cpu().numpy()\n",
    "         \n",
    "        # Compute RMSE on scaled values\n",
    "        y_valid = y_valid.values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_valid) ** 2))\n",
    "        print(f\"Fold {fold} - RMSE: {rmse}\")\n",
    "\n",
    "        # Save out-of-fold predictions\n",
    "        oof.loc[val_index, f'{target}_true'] = y_valid\n",
    "        oof.loc[val_index, f'{target}_pred'] = y_pred\n",
    "        oof.loc[val_index, 'fold'] = fold\n",
    "\n",
    "        # Save RMSE to the list\n",
    "        oof_scores.append(rmse)\n",
    "\n",
    "    # Print the list of oof scores and average oof score\n",
    "    print(\"List of oof scores:\", oof_scores)\n",
    "    print(\"Average oof score:\", np.mean(oof_scores))\n",
    "\n",
    "    return trained_models, lgbm_encoders, oof, oof_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "trained_models1, lgbm_encoders1, oof1, oof_scores1 = train_models(\n",
    "    df_train, \n",
    "    numerical_cols, \n",
    "    categorical_cols, \n",
    "    target='utility_agent1',\n",
    "    split_list=split1,\n",
    ")\n",
    "# save oof predictions\n",
    "oof1.to_parquet(path_results / f'oof_nn-gandalf_{fs_type}_cv1.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "trained_models2, lgbm_encoders2, oof2, oof_scores2 = train_models(\n",
    "    df_train, \n",
    "    numerical_cols, \n",
    "    categorical_cols, \n",
    "    target='utility_agent1',\n",
    "    split_list=split2,\n",
    ")\n",
    "# save oof predictions\n",
    "oof2.to_parquet(path_results / f'oof_nn-gandalf_{fs_type}_cv2.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "trained_models3, lgbm_encoders3, oof3, oof_scores3 = train_models(\n",
    "    df_train, \n",
    "    numerical_cols, \n",
    "    categorical_cols, \n",
    "    target='utility_agent1',\n",
    "    split_list=split3,\n",
    ")\n",
    "# save oof predictions\n",
    "oof3.to_parquet(path_results / f'oof_nn-gandalf_{fs_type}_cv3.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = [*trained_models1, *trained_models2, *trained_models3]\n",
    "lgbm_encoders = [*lgbm_encoders1, *lgbm_encoders2, *lgbm_encoders3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of OOF scores for each CV fold\n",
    "print(\"CV1 OOF Scores:\")\n",
    "for score in oof_scores1:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "print(f\"Average CV1 Score: {sum(oof_scores1)/len(oof_scores1):.4f}\")\n",
    "    \n",
    "print(\"\\nCV2 OOF Scores:\")  \n",
    "for score in oof_scores2:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "print(f\"Average CV2 Score: {sum(oof_scores2)/len(oof_scores2):.4f}\")\n",
    "    \n",
    "print(\"\\nCV3 OOF Scores:\")\n",
    "for score in oof_scores3:\n",
    "    print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(oof1['utility_agent1_true'], alpha=0.5, label='Target')\n",
    "plt.hist(oof1['utility_agent1_pred'], alpha=0.5, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "sys.path.append(\"../data/raw/\")\n",
    "import kaggle_evaluation.mcts_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame, sample_sub: pl.DataFrame):\n",
    "    # Convert Polars DataFrame to Pandas DataFrame\n",
    "    test_pd = test.to_pandas()\n",
    "    \n",
    "    # Process the test data\n",
    "    test_processed = process_test_data(\n",
    "        test_pd,\n",
    "        numerical_cols,\n",
    "        categorical_cols,\n",
    "        encoder,\n",
    "        scaler\n",
    "    )\n",
    "        \n",
    "    # Initialize an array to store predictions\n",
    "    predictions = np.zeros(len(test_pd))\n",
    "    \n",
    "    # Make predictions using each trained model\n",
    "    for lgbm_encoder,model in zip(lgbm_encoders, trained_models):\n",
    "\n",
    "        # Select relevant features\n",
    "        X_test_num = test_processed[numerical_cols].copy()\n",
    "        X_test_cat = test_processed[categorical_cols].copy()\n",
    "\n",
    "        # Add LGBM encoder leaves features\n",
    "        lgbm_features = lgbm_encoder.transform(\n",
    "            test_processed[numerical_cols+categorical_cols]\n",
    "        )\n",
    "        X_test_cat = pd.concat([X_test_cat, lgbm_features], axis=1)\n",
    "        _categorical_cols = categorical_cols + lgbm_encoder.new_columns\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_num_tensor = torch.tensor(\n",
    "                X_test_num[numerical_cols].values, dtype=torch.float32\n",
    "            )\n",
    "            X_test_cat_tensor = torch.tensor(\n",
    "                X_test_cat[_categorical_cols].values, dtype=torch.int32\n",
    "            )\n",
    "            batch_predictions = model(\n",
    "                X_test_num_tensor, X_test_cat_tensor\n",
    "            ).cpu().numpy().flatten()\n",
    "\n",
    "        predictions += batch_predictions\n",
    "    \n",
    "    # Average the predictions\n",
    "    predictions /= len(trained_models)\n",
    "    \n",
    "    # Create the submission DataFrame\n",
    "    submission = sample_sub.with_columns(pl.Series(\"utility_agent1\", predictions))\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pl.read_csv('../data/raw/test.csv')\n",
    "sample_sub = pl.read_csv('../data/raw/sample_submission.csv')\n",
    "predict(test, sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.mcts_inference_server.MCTSInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            '../data/raw/test.csv',\n",
    "            '../data/raw/sample_submission.csv'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_save = {\n",
    "    'models': [model.state_dict() for model in trained_models],\n",
    "    'models_hparams': [model.hparams for model in trained_models],\n",
    "    'numerical_cols': numerical_cols,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'encoder': encoder,\n",
    "    'scaler': scaler,\n",
    "    'lgbm_encoders': lgbm_encoders\n",
    "}\n",
    "torch.save(torch_save, f'./nn-gandalf_predict_{fs_type}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-strength-variants-kSTIVMm8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
