{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from preproc import preprocess_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some paths\n",
    "path_raw = Path(\"../data/raw\")\n",
    "path_processed = Path(\"../data/processed\")\n",
    "path_results = Path(\"../data/results\")\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_csv(path_raw / \"train.csv\")\n",
    "df_test = pd.read_csv(path_raw / \"test.csv\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "df_train, df_test, numerical_cols, categorical_cols = preprocess_data(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    scale_utility=True\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Numerical Columns:\", len(numerical_cols))\n",
    "print(\"Categorical Columns:\", len(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['utility_agent1_scaled'], bins=100)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Utility Agent 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[numerical_cols] = df_train[numerical_cols].astype(np.float32)\n",
    "df_train[categorical_cols] = df_train[categorical_cols].astype(np.int32)\n",
    "\n",
    "cat_input_dims = df_train[categorical_cols].nunique(axis=0).values.tolist()\n",
    "print(cat_input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns of df_train\n",
    "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
    "\n",
    "# Print a message to confirm the scaling\n",
    "print(\"Numerical columns have been scaled using StandardScaler.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "            num_input_dim: int,\n",
    "            cat_input_dims: list[int],\n",
    "            output_dim: int,\n",
    "            layers: str,\n",
    "            dropout: float,\n",
    "            learning_rate: float = 1e-3,\n",
    "            weight_decay: float = 1e-5,\n",
    "            initialization: str = 'kaiming_normal',\n",
    "            embedding_dim: Optional[List[int]] = None,\n",
    "            target_range: Optional[Union[Tuple[float, float], List[Tuple[float, float]]]] = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.target_range = target_range\n",
    "\n",
    "        # Initialize embedding dimensions if not provided\n",
    "        if embedding_dim is None:\n",
    "            # Rule of thumb: min(50, num_unique // 2 + 1) for each categorical feature\n",
    "            embedding_dim = [min(50, int(1 + np.ceil(np.sqrt(dim)))) for dim in cat_input_dims]\n",
    "\n",
    "        elif len(embedding_dim) != len(cat_input_dims):\n",
    "            raise ValueError(\"Length of embedding_dim must match number of categorical features.\")\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create embedding layers\n",
    "        self.create_embeddings(cat_input_dims, embedding_dim)\n",
    "\n",
    "        # Create backbone layers\n",
    "        self.create_backbone(num_input_dim, layers)\n",
    "\n",
    "        # Create head layers\n",
    "        self.create_head(output_dim)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.initialization = initialization\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # Initialize lists to store validation outputs\n",
    "        self.validation_targets = []\n",
    "        self.validation_predictions = []\n",
    "\n",
    "    def create_embeddings(self, cat_input_dims: list[int], embedding_dim: list[int]):\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(dim, emb_dim) for dim, emb_dim in zip(cat_input_dims, embedding_dim)]\n",
    "        )\n",
    "\n",
    "    def create_backbone(self, num_input_dim: int, layers: str):\n",
    "        # Calculate total input dimension after embeddings\n",
    "        total_embedding_dim = sum(self.embedding_dim)\n",
    "        total_input_dim = num_input_dim + total_embedding_dim\n",
    "\n",
    "        # Parse layers string\n",
    "        layer_sizes = [int(size) for size in layers.split('-')]\n",
    "\n",
    "        # Create backbone network layers\n",
    "        backbone_layers = []\n",
    "        prev_size = total_input_dim\n",
    "        for size in layer_sizes:\n",
    "            backbone_layers.extend([\n",
    "                nn.BatchNorm1d(prev_size),\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.hparams.dropout),\n",
    "            ])\n",
    "            prev_size = size\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        self.backbone_output_size = prev_size\n",
    "\n",
    "    def create_head(self, output_dim: int):\n",
    "        # Output layer\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.backbone_output_size),\n",
    "            nn.Linear(self.backbone_output_size, output_dim)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if self.initialization == 'kaiming_normal':\n",
    "                    nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                elif self.initialization == 'kaiming_uniform':\n",
    "                    nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "                elif self.initialization == 'xavier_normal':\n",
    "                    nn.init.xavier_normal_(module.weight)\n",
    "                elif self.initialization == 'xavier_uniform':\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported initialization method: {self.initialization}\")\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Process categorical variables\n",
    "        embedded = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        \n",
    "        # Concatenate numerical and embedded categorical features\n",
    "        x = torch.cat([x_num, embedded], dim=1)\n",
    "        \n",
    "        # Pass through backbone\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Pass through head\n",
    "        x = self.head(x)\n",
    "\n",
    "        if self.target_range is not None:\n",
    "            if isinstance(self.target_range, list):\n",
    "                # Assuming multiple targets\n",
    "                # Apply sigmoid for each target and scale\n",
    "                min_vals, max_vals = zip(*self.target_range)\n",
    "                min_vals = torch.tensor(min_vals, device=x.device).view(1, -1)\n",
    "                max_vals = torch.tensor(max_vals, device=x.device).view(1, -1)\n",
    "                x = torch.sigmoid(x) * (max_vals - min_vals) + min_vals\n",
    "            else:\n",
    "                # min_val, max_val = self.target_range\n",
    "                # x = torch.sigmoid(x) * (max_val - min_val) + min_val\n",
    "                x = torch.tanh(x)\n",
    "\n",
    "        return x.squeeze(-1)  # Squeeze the last dimension to match target shape\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        # Store targets and predictions for later use\n",
    "        self.validation_targets.append(y)\n",
    "        self.validation_predictions.append(y_hat)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        if len(batch) == 2:\n",
    "            x_num, x_cat = batch\n",
    "        elif len(batch) == 3:\n",
    "            x_num, x_cat, _ = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        return y_hat\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all targets and predictions\n",
    "        y = torch.cat(self.validation_targets)\n",
    "        y_hat = torch.cat(self.validation_predictions)\n",
    "        rmse = torch.sqrt(F.mse_loss(y_hat, y))\n",
    "        self.log('val_rmse', rmse, prog_bar=True)\n",
    "        # Clear the lists for next epoch\n",
    "        self.validation_targets.clear()\n",
    "        self.validation_predictions.clear()\n",
    "                \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.2,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=True,\n",
    "            base_momentum=0.85,\n",
    "            max_momentum=0.95,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_rmse',\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "class LearningRateMonitor(pl.Callback):\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if batch_idx % 100 == 0:  # Log every 100 batches\n",
    "            lr = pl_module.optimizers().param_groups[0]['lr']\n",
    "            pl_module.log('learning_rate', lr, prog_bar=True)   \n",
    "\n",
    "class BestValRMSELogger(pl.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_val_rmse = float('inf')\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        current_val_rmse = trainer.callback_metrics.get('val_rmse')\n",
    "        if current_val_rmse is not None:\n",
    "            self.best_val_rmse = min(self.best_val_rmse, current_val_rmse)\n",
    "            pl_module.log('best_val_rmse', self.best_val_rmse, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Define the column for stratified or group k-fold\n",
    "groups_col = 'GameRulesetName'\n",
    "gkf = GroupKFold(n_splits=num_folds)\n",
    "split_list = gkf.split(df_train, groups=df_train[groups_col])\n",
    "\n",
    "trained_models = []\n",
    "oof = pd.DataFrame(index=df_train.index, columns=['utility_agent1_true', 'utility_agent1_pred'])\n",
    "oof_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, val_index) in enumerate(split_list, 1):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    if fold == 1: \n",
    "        continue\n",
    "    \n",
    "    # Split the data\n",
    "    train, valid = df_train.iloc[train_index], df_train.iloc[val_index]\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(train[numerical_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(train[categorical_cols].values, dtype=torch.int32),\n",
    "        torch.tensor(train['utility_agent1'].values, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    valid_dataset = TensorDataset(\n",
    "        torch.tensor(valid[numerical_cols].values, dtype=torch.float32),\n",
    "        torch.tensor(valid[categorical_cols].values, dtype=torch.int32),\n",
    "        torch.tensor(valid['utility_agent1'].values, dtype=torch.float32)\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    model = MLP(\n",
    "        num_input_dim=len(numerical_cols),\n",
    "        cat_input_dims=cat_input_dims,\n",
    "        output_dim=1,\n",
    "        layers=\"768-384-192\",\n",
    "        dropout=0.1,  # Updated dropout\n",
    "        learning_rate=1e-2,  # Updated max learning rate\n",
    "        weight_decay=5e-5,  # Updated weight decay\n",
    "        initialization='kaiming_uniform',\n",
    "        target_range=(-1, 1),\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        min_epochs=20, \n",
    "        max_epochs=100, \n",
    "        #deterministic=True,\n",
    "        accelerator=\"mps\", \n",
    "        #devices=1,\n",
    "        callbacks=[\n",
    "            early_stop_callback, \n",
    "            LearningRateMonitor(), \n",
    "            BestValRMSELogger(),\n",
    "            ModelCheckpoint(monitor='val_rmse', mode='min', save_top_k=1),\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model, \n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "    )\n",
    "\n",
    "    # Load the best model\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    model = MLP.load_from_checkpoint(best_model_path)\n",
    "\n",
    "    # Predict on validation set using trainer.predict with the prediction DataLoader\n",
    "    predictions = trainer.predict(model, dataloaders=valid_loader)\n",
    "    y_pred = torch.cat(predictions).squeeze().cpu().numpy()\n",
    "     \n",
    "    # Compute RMSE on scaled values\n",
    "    y_valid = valid['utility_agent1'].values\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_valid) ** 2))\n",
    "    print(f\"Fold {fold} - RMSE: {rmse}\")\n",
    "\n",
    "    # Save out-of-fold predictions\n",
    "    oof.loc[val_index, 'utility_agent1_true'] = y_valid\n",
    "    oof.loc[val_index, 'utility_agent1_pred'] = y_pred\n",
    "\n",
    "    # Save RMSE to the list\n",
    "    oof_scores.append(rmse)\n",
    "\n",
    "# Print the list of oof scores and average oof score\n",
    "print(\"List of oof scores:\", oof_scores)\n",
    "print(\"Average oof score:\", np.mean(oof_scores))\n",
    "\n",
    "oof.to_csv(path_results / 'oof_mlp.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(oof['utility_agent1_true'], alpha=0.5, label='Target')\n",
    "plt.hist(oof['utility_agent1_pred'], alpha=0.5, label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-strength-variants-kSTIVMm8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
