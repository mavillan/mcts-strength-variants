{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66b195c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:43.686950Z",
     "iopub.status.busy": "2024-11-11T22:09:43.686638Z",
     "iopub.status.idle": "2024-11-11T22:09:46.502042Z",
     "shell.execute_reply": "2024-11-11T22:09:46.501751Z"
    },
    "papermill": {
     "duration": 2.824836,
     "end_time": "2024-11-11T22:09:46.502810",
     "exception": false,
     "start_time": "2024-11-11T22:09:43.677974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/jw/hkbjn4190p76_0dtnrpy7zwm0000gn/T/ipykernel_13225/2436190809.py:22: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "TensorFlow version: 2.15.0, GPU = True\n",
      "DeepTables version: 0.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 08:44:32.060193: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-11-20 08:44:32.060225: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-11-20 08:44:32.060229: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-11-20 08:44:32.060255: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-20 08:44:32.060270: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from colorama import Fore, Style\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf, deeptables as dt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from deeptables.models import DeepTable, ModelConfig\n",
    "from deeptables.models import deepnets\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('TensorFlow version:',tf.__version__+',',\n",
    "      'GPU =',tf.test.is_gpu_available())\n",
    "print('DeepTables version:',dt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d2e4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.505914Z",
     "iopub.status.busy": "2024-11-11T22:09:46.505718Z",
     "iopub.status.idle": "2024-11-11T22:09:46.507989Z",
     "shell.execute_reply": "2024-11-11T22:09:46.507720Z"
    },
    "papermill": {
     "duration": 0.004355,
     "end_time": "2024-11-11T22:09:46.508679",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.504324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d88b9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.511099Z",
     "iopub.status.busy": "2024-11-11T22:09:46.511001Z",
     "iopub.status.idle": "2024-11-11T22:09:46.581656Z",
     "shell.execute_reply": "2024-11-11T22:09:46.581387Z"
    },
    "papermill": {
     "duration": 0.072679,
     "end_time": "2024-11-11T22:09:46.582400",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.509721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.001 to 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\n",
    "LR_START = 1e-4\n",
    "LR_MAX = 1e-3\n",
    "LR_MIN = 1e-3\n",
    "LR_RAMPUP_EPOCHS = 1\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "EPOCHS = 7\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "        cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "lr_y = [lrfn(x) for x in rng]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rng, lr_y, '-o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('LR')\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n",
    "      format(lr_y[0], max(lr_y), lr_y[-1]))\n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee65134d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.586109Z",
     "iopub.status.busy": "2024-11-11T22:09:46.585991Z",
     "iopub.status.idle": "2024-11-11T22:09:46.589274Z",
     "shell.execute_reply": "2024-11-11T22:09:46.589005Z"
    },
    "papermill": {
     "duration": 0.005523,
     "end_time": "2024-11-11T22:09:46.589970",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.584447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = '../data/raw/train.csv'\n",
    "    split_agent_features = True\n",
    "    scaler = MinMaxScaler()  # Scaler or None\n",
    "    \n",
    "    nn = True    \n",
    "    folds = 5\n",
    "    epochs = 7\n",
    "    batch_size = 128\n",
    "    LR_Scheduler = [LR_Scheduler]\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    conf = ModelConfig(\n",
    "        auto_imputation=False,\n",
    "        auto_discrete=False,\n",
    "        auto_discard_unique=True,\n",
    "        categorical_columns='auto',\n",
    "        apply_gbm_features=True,\n",
    "        fixed_embedding_dim=True,\n",
    "        embeddings_output_dim=4,\n",
    "        embedding_dropout=0.2,\n",
    "        # nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n",
    "        # nets = ['dnn_nets'] + ['fm_nets'],\n",
    "        nets =['dnn_nets'] + ['cin_nets'],\n",
    "        dnn_params={\n",
    "            'hidden_units': ((1024, 0.0, True),\n",
    "                             (512, 0.0, True),\n",
    "                             (256, 0.0, True),\n",
    "                             (128, 0.0, True)),\n",
    "            'dnn_activation': 'relu',\n",
    "        },\n",
    "        stacking_op='concat',\n",
    "        output_use_bias=False,\n",
    "        optimizer=optimizer,\n",
    "        task='regression',\n",
    "        loss='auto',\n",
    "        metrics=[\"RootMeanSquaredError\"],\n",
    "        earlystopping_patience=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "377b913a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.592752Z",
     "iopub.status.busy": "2024-11-11T22:09:46.592657Z",
     "iopub.status.idle": "2024-11-11T22:09:46.605191Z",
     "shell.execute_reply": "2024-11-11T22:09:46.604941Z"
    },
    "papermill": {
     "duration": 0.014755,
     "end_time": "2024-11-11T22:09:46.605941",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.591186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "constant_cols = pd.read_csv('../data/constant_columns.csv').columns.to_list()\n",
    "target_col = 'utility_agent1'\n",
    "game_col = 'GameRulesetName'\n",
    "game_rule_cols = ['EnglishRules', 'LudRules']\n",
    "output_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\n",
    "dropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\n",
    "agent_cols = ['agent1', 'agent2']\n",
    "\n",
    "def preprocess_data(df): \n",
    "    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n",
    "    if CFG.split_agent_features:\n",
    "        for col in agent_cols:\n",
    "            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n",
    "    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n",
    "    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n",
    "    df = df.to_pandas()\n",
    "    df[\"utility_agent1_rank\"] = (\n",
    "        df[\"utility_agent1\"].rank(method='dense', ascending=True).astype(int)\n",
    "    )\n",
    "    print(f'Data shape: {df.shape}\\n')\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['category']).columns.tolist()\n",
    "    non_cat_cols = df.select_dtypes(exclude=['category']).columns.tolist()\n",
    "    num_cols = [num for num in non_cat_cols if num not in [target_col, game_col, \"utility_agent1_rank\"]]\n",
    "    return df, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b1c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1 = pickle.load(open('../data/splits/cv1_Game.pkl', 'rb'))\n",
    "\n",
    "# split1 = pickle.load(open('../data/splits/cv1_GameRulesetName.pkl', 'rb'))\n",
    "# split2 = pickle.load(open('../data/splits/cv2_GameRulesetName.pkl', 'rb'))\n",
    "# split3 = pickle.load(open('../data/splits/cv3_GameRulesetName.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe2d07f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.608975Z",
     "iopub.status.busy": "2024-11-11T22:09:46.608863Z",
     "iopub.status.idle": "2024-11-11T22:09:46.614536Z",
     "shell.execute_reply": "2024-11-11T22:09:46.614310Z"
    },
    "papermill": {
     "duration": 0.007913,
     "end_time": "2024-11-11T22:09:46.615235",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.607322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(data, cat_cols, num_cols, scaler):\n",
    "    split_list = split1.copy()\n",
    "\n",
    "    X = data.drop([target_col, \"utility_agent1_rank\", game_col], axis=1)\n",
    "    y = data[target_col]\n",
    "    oof = np.zeros(len(data))\n",
    "    oof_scores = list()\n",
    "    nn_models = []\n",
    "\n",
    "    target = \"utility_agent1\"\n",
    "    oof_df = pd.DataFrame(\n",
    "        index=data.index,\n",
    "        columns=[f'{target}_true', f'{target}_pred', 'fold']\n",
    "    )\n",
    "    \n",
    "    print('nn = '+str(CFG.nn),'\\n')\n",
    "    \n",
    "    for fi, (train_idx, valid_idx) in enumerate(split_list, 1):\n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Fold {fi}/{CFG.folds} ...\")\n",
    "        print(\"#\"*25)\n",
    "        \n",
    "        os.makedirs(f\"../data/working/nn_models/fold{fi}\", exist_ok=True)\n",
    "\n",
    "        if CFG.nn == True:\n",
    "            print('\\n',\"nn model training.\",'\\n')\n",
    "            K.clear_session()\n",
    "            nn_model = DeepTable(config=CFG.conf)\n",
    "            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n",
    "                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n",
    "                      callbacks=CFG.LR_Scheduler,\n",
    "                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n",
    "            nn_models.append(nn_model)\n",
    "            \n",
    "            # Save model\n",
    "            # nn_model.save(f'../data/working/nn_models/fold{fi}')\n",
    "        \n",
    "            # Avoid some errors\n",
    "            with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "                for j, var in enumerate(CFG.optimizer.weights):\n",
    "                    name = 'variable{}'.format(j)\n",
    "                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n",
    "            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n",
    "\n",
    "            oof_preds = nn_model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n",
    "            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n",
    "            oof_scores.append(rmse)\n",
    "            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi} | rmse: {rmse}\\n')\n",
    "\n",
    "            oof[valid_idx] = oof_preds\n",
    "            oof_df.loc[valid_idx, f'{target}_true'] = y.iloc[valid_idx]\n",
    "            oof_df.loc[valid_idx, f'{target}_pred'] = oof_preds \n",
    "            oof_df.loc[valid_idx, 'fold'] = fi\n",
    "        else:\n",
    "            raise ValueError(\"Neural network model not enabled in CFG.\")\n",
    "    \n",
    "    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n",
    "    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n",
    "    print(f'{Fore.BLUE}{Style.BRIGHT}OOF scores: {oof_scores} (avg: {np.mean(oof_scores):.4f})\\n')\n",
    "    plot_model(nn_model.get_model().model)\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    oof_df.to_csv('../data/results/oof_dt-nn_cv1.csv', index=False)\n",
    "        \n",
    "    return nn_models\n",
    "\n",
    "\n",
    "def infer(data, nn_models, ctb_models, num_cols, scaler):\n",
    "    return np.mean(\n",
    "        [model.predict(data, verbose=1, batch_size=512).flatten()\n",
    "         for model in nn_models\n",
    "        ],\n",
    "        axis=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea419cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:46.618068Z",
     "iopub.status.busy": "2024-11-11T22:09:46.617965Z",
     "iopub.status.idle": "2024-11-11T22:09:47.894498Z",
     "shell.execute_reply": "2024-11-11T22:09:47.894016Z"
    },
    "papermill": {
     "duration": 1.27899,
     "end_time": "2024-11-11T22:09:47.895473",
     "exception": false,
     "start_time": "2024-11-11T22:09:46.616483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (233234, 599)\n",
      "\n",
      "Scaling 588 numerical cols.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = CFG.scaler\n",
    "train_df = pl.read_csv(CFG.train_path)\n",
    "train_df, cat_cols, num_cols = preprocess_data(train_df)\n",
    "if scaler is not None:\n",
    "    print(f'Scaling {len(num_cols)} numerical cols.\\n')\n",
    "    train_df[num_cols] = scaler.fit_transform(train_df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "751724a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T22:09:47.899626Z",
     "iopub.status.busy": "2024-11-11T22:09:47.899489Z",
     "iopub.status.idle": "2024-11-11T22:56:16.839760Z",
     "shell.execute_reply": "2024-11-11T22:56:16.839341Z"
    },
    "papermill": {
     "duration": 2788.942732,
     "end_time": "2024-11-11T22:56:16.840573",
     "exception": false,
     "start_time": "2024-11-11T22:09:47.897841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn = True \n",
      "\n",
      "#########################\n",
      "### Fold 1/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-20 08:44:34 I deeptables.m.deeptable.py 337 - X.Shape=(190558, 596), y.Shape=(190558,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x32460a6b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-20 08:44:34 I deeptables.m.deeptable.py 338 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-20 08:44:35 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 08:44:35 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "11-20 08:44:35 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.33132100105285645s\n",
      "11-20 08:44:35 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "11-20 08:44:35 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.12087392807006836s\n",
      "11-20 08:44:35 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "11-20 08:44:36 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.068297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16860\n",
      "[LightGBM] [Info] Number of data points in the train set: 190558, number of used features: 578\n",
      "[LightGBM] [Info] Start training from score 0.040548\n",
      "11-20 08:44:39 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.9604530334472656s\n",
      "11-20 08:44:40 I deeptables.m.preprocessor.py 196 - fit_transform taken 4.993121862411499s\n",
      "11-20 08:44:40 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 08:44:40 I deeptables.m.preprocessor.py 249 - transform_X taken 0.2667851448059082s\n",
      "11-20 08:44:40 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "11-20 08:44:40 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0001308917999267578s\n",
      "11-20 08:44:40 I deeptables.m.deeptable.py 353 - Training...\n",
      "11-20 08:44:40 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:1, mode:min\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-20 08:44:40 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 08:44:40.582722: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-20 08:44:40.582739: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 08:44:41 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 08:44:41 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "11-20 08:44:42 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (570)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1002)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1002), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-20 08:44:42 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 08:44:45.758333: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 86s - loss: 0.1323 - root_mean_squared_error: 0.3638 - val_loss: 0.2066 - val_root_mean_squared_error: 0.4545 - lr: 1.0000e-04 - 86s/epoch - 58ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 77s - loss: 0.0891 - root_mean_squared_error: 0.2985 - val_loss: 0.1988 - val_root_mean_squared_error: 0.4459 - lr: 0.0010 - 77s/epoch - 52ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 77s - loss: 0.0697 - root_mean_squared_error: 0.2640 - val_loss: 0.1882 - val_root_mean_squared_error: 0.4338 - lr: 0.0010 - 77s/epoch - 52ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 78s - loss: 0.0607 - root_mean_squared_error: 0.2463 - val_loss: 0.1896 - val_root_mean_squared_error: 0.4354 - lr: 0.0010 - 78s/epoch - 53ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 78s - loss: 0.0541 - root_mean_squared_error: 0.2326 - val_loss: 0.1893 - val_root_mean_squared_error: 0.4351 - lr: 0.0010 - 78s/epoch - 53ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 79s - loss: 0.0488 - root_mean_squared_error: 0.2208 - val_loss: 0.1865 - val_root_mean_squared_error: 0.4319 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1488/1488 - 79s - loss: 0.0440 - root_mean_squared_error: 0.2098 - val_loss: 0.1887 - val_root_mean_squared_error: 0.4344 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "11-20 08:53:58 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-20 08:53:58 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "11-20 08:53:58 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241120084434_cin_nets_dnn_nets/cin_nets+dnn_nets.h5\n",
      "11-20 08:53:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-20 08:53:58 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 08:53:58 I deeptables.m.preprocessor.py 249 - transform_X taken 0.2668449878692627s\n",
      "11-20 08:53:58 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-20 08:53:58 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "84/84 [==============================] - 3s 25ms/step\n",
      "11-20 08:54:01 I deeptables.m.deeptable.py 559 - predict_proba taken 3.0979111194610596s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 1 | rmse: 0.4343000054359436\n",
      "\n",
      "#########################\n",
      "### Fold 2/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-20 08:54:02 I deeptables.m.deeptable.py 337 - X.Shape=(196422, 596), y.Shape=(196422,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x32460a6b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-20 08:54:02 I deeptables.m.deeptable.py 338 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-20 08:54:03 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 08:54:03 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "11-20 08:54:04 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.3211209774017334s\n",
      "11-20 08:54:04 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "11-20 08:54:04 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.1266167163848877s\n",
      "11-20 08:54:04 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "11-20 08:54:04 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17067\n",
      "[LightGBM] [Info] Number of data points in the train set: 196422, number of used features: 590\n",
      "[LightGBM] [Info] Start training from score 0.047370\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.955833911895752s\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 196 - fit_transform taken 5.09991979598999s\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 249 - transform_X taken 0.21745991706848145s\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "11-20 08:54:08 I deeptables.m.preprocessor.py 236 - transform_y taken 4.792213439941406e-05s\n",
      "11-20 08:54:09 I deeptables.m.deeptable.py 353 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-20 08:54:09 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 08:54:09 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 08:54:09 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "11-20 08:54:10 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (583)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1015)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1015), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-20 08:54:10 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 89s - loss: 0.1550 - root_mean_squared_error: 0.3937 - val_loss: 0.2577 - val_root_mean_squared_error: 0.5077 - lr: 1.0000e-04 - 89s/epoch - 58ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 81s - loss: 0.0997 - root_mean_squared_error: 0.3158 - val_loss: 0.2301 - val_root_mean_squared_error: 0.4797 - lr: 0.0010 - 81s/epoch - 53ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 81s - loss: 0.0752 - root_mean_squared_error: 0.2742 - val_loss: 0.2192 - val_root_mean_squared_error: 0.4681 - lr: 0.0010 - 81s/epoch - 53ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 80s - loss: 0.0671 - root_mean_squared_error: 0.2590 - val_loss: 0.2176 - val_root_mean_squared_error: 0.4665 - lr: 0.0010 - 80s/epoch - 52ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 80s - loss: 0.0604 - root_mean_squared_error: 0.2457 - val_loss: 0.2186 - val_root_mean_squared_error: 0.4675 - lr: 0.0010 - 80s/epoch - 52ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 80s - loss: 0.0541 - root_mean_squared_error: 0.2325 - val_loss: 0.2123 - val_root_mean_squared_error: 0.4608 - lr: 0.0010 - 80s/epoch - 52ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1534/1534 - 80s - loss: 0.0489 - root_mean_squared_error: 0.2212 - val_loss: 0.2123 - val_root_mean_squared_error: 0.4608 - lr: 0.0010 - 80s/epoch - 52ms/step\n",
      "11-20 09:03:41 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-20 09:03:41 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "11-20 09:03:41 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241120085402_cin_nets_dnn_nets/cin_nets+dnn_nets.h5\n",
      "11-20 09:03:42 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-20 09:03:42 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:03:42 I deeptables.m.preprocessor.py 249 - transform_X taken 0.2496628761291504s\n",
      "11-20 09:03:42 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-20 09:03:42 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "72/72 [==============================] - 2s 26ms/step\n",
      "11-20 09:03:44 I deeptables.m.deeptable.py 559 - predict_proba taken 2.594003915786743s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 2 | rmse: 0.4611000120639801\n",
      "\n",
      "#########################\n",
      "### Fold 3/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-20 09:03:45 I deeptables.m.deeptable.py 337 - X.Shape=(193178, 596), y.Shape=(193178,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x32460a6b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-20 09:03:45 I deeptables.m.deeptable.py 338 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-20 09:03:46 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 09:03:46 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "11-20 09:03:47 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.2917907238006592s\n",
      "11-20 09:03:47 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "11-20 09:03:47 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.12428522109985352s\n",
      "11-20 09:03:47 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "11-20 09:03:47 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16966\n",
      "[LightGBM] [Info] Number of data points in the train set: 193178, number of used features: 590\n",
      "[LightGBM] [Info] Start training from score 0.045943\n",
      "11-20 09:03:51 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.8715479373931885s\n",
      "11-20 09:03:51 I deeptables.m.preprocessor.py 196 - fit_transform taken 4.993305921554565s\n",
      "11-20 09:03:51 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:03:52 I deeptables.m.preprocessor.py 249 - transform_X taken 0.24213290214538574s\n",
      "11-20 09:03:52 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "11-20 09:03:52 I deeptables.m.preprocessor.py 236 - transform_y taken 0.0002079010009765625s\n",
      "11-20 09:03:52 I deeptables.m.deeptable.py 353 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-20 09:03:52 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:03:52 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:03:52 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "11-20 09:03:53 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (582)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1014)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1014), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-20 09:03:53 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 88s - loss: 0.1535 - root_mean_squared_error: 0.3918 - val_loss: 0.2369 - val_root_mean_squared_error: 0.4867 - lr: 1.0000e-04 - 88s/epoch - 59ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 80s - loss: 0.0978 - root_mean_squared_error: 0.3127 - val_loss: 0.2102 - val_root_mean_squared_error: 0.4585 - lr: 0.0010 - 80s/epoch - 53ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 80s - loss: 0.0753 - root_mean_squared_error: 0.2744 - val_loss: 0.2112 - val_root_mean_squared_error: 0.4595 - lr: 0.0010 - 80s/epoch - 53ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 79s - loss: 0.0659 - root_mean_squared_error: 0.2567 - val_loss: 0.2133 - val_root_mean_squared_error: 0.4618 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 79s - loss: 0.0588 - root_mean_squared_error: 0.2426 - val_loss: 0.2070 - val_root_mean_squared_error: 0.4549 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 80s - loss: 0.0530 - root_mean_squared_error: 0.2302 - val_loss: 0.2079 - val_root_mean_squared_error: 0.4559 - lr: 0.0010 - 80s/epoch - 53ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1509/1509 - 79s - loss: 0.0473 - root_mean_squared_error: 0.2176 - val_loss: 0.2092 - val_root_mean_squared_error: 0.4574 - lr: 0.0010 - 79s/epoch - 52ms/step\n",
      "11-20 09:13:19 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-20 09:13:19 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "11-20 09:13:19 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241120090345_cin_nets_dnn_nets/cin_nets+dnn_nets.h5\n",
      "11-20 09:13:20 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-20 09:13:20 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:13:20 I deeptables.m.preprocessor.py 249 - transform_X taken 0.24573588371276855s\n",
      "11-20 09:13:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-20 09:13:20 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "79/79 [==============================] - 2s 25ms/step\n",
      "11-20 09:13:23 I deeptables.m.deeptable.py 559 - predict_proba taken 2.666457176208496s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 3 | rmse: 0.45739999413490295\n",
      "\n",
      "#########################\n",
      "### Fold 4/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-20 09:13:23 I deeptables.m.deeptable.py 337 - X.Shape=(190888, 596), y.Shape=(190888,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x32460a6b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-20 09:13:23 I deeptables.m.deeptable.py 338 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-20 09:13:25 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 09:13:25 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "11-20 09:13:25 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.2815580368041992s\n",
      "11-20 09:13:25 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "11-20 09:13:25 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.13254117965698242s\n",
      "11-20 09:13:25 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "11-20 09:13:26 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16898\n",
      "[LightGBM] [Info] Number of data points in the train set: 190888, number of used features: 595\n",
      "[LightGBM] [Info] Start training from score 0.047922\n",
      "11-20 09:13:29 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.792060136795044s\n",
      "11-20 09:13:30 I deeptables.m.preprocessor.py 196 - fit_transform taken 4.84060001373291s\n",
      "11-20 09:13:30 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:13:30 I deeptables.m.preprocessor.py 249 - transform_X taken 0.2655448913574219s\n",
      "11-20 09:13:30 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "11-20 09:13:30 I deeptables.m.preprocessor.py 236 - transform_y taken 5.078315734863281e-05s\n",
      "11-20 09:13:30 I deeptables.m.deeptable.py 353 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-20 09:13:30 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:13:31 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:13:31 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "11-20 09:13:31 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (587)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1019)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1019), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-20 09:13:31 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 88s - loss: 0.1123 - root_mean_squared_error: 0.3352 - val_loss: 0.2200 - val_root_mean_squared_error: 0.4690 - lr: 1.0000e-04 - 88s/epoch - 59ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 80s - loss: 0.0891 - root_mean_squared_error: 0.2986 - val_loss: 0.2038 - val_root_mean_squared_error: 0.4515 - lr: 0.0010 - 80s/epoch - 54ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 79s - loss: 0.0697 - root_mean_squared_error: 0.2640 - val_loss: 0.2035 - val_root_mean_squared_error: 0.4511 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 79s - loss: 0.0608 - root_mean_squared_error: 0.2466 - val_loss: 0.2051 - val_root_mean_squared_error: 0.4528 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 80s - loss: 0.0541 - root_mean_squared_error: 0.2326 - val_loss: 0.2056 - val_root_mean_squared_error: 0.4534 - lr: 0.0010 - 80s/epoch - 54ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 79s - loss: 0.0486 - root_mean_squared_error: 0.2205 - val_loss: 0.2029 - val_root_mean_squared_error: 0.4505 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1491/1491 - 79s - loss: 0.0438 - root_mean_squared_error: 0.2092 - val_loss: 0.2058 - val_root_mean_squared_error: 0.4537 - lr: 0.0010 - 79s/epoch - 53ms/step\n",
      "11-20 09:22:56 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-20 09:22:56 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "11-20 09:22:57 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241120091323_cin_nets_dnn_nets/cin_nets+dnn_nets.h5\n",
      "11-20 09:22:58 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-20 09:22:58 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:22:58 I deeptables.m.preprocessor.py 249 - transform_X taken 0.26108217239379883s\n",
      "11-20 09:22:58 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-20 09:22:58 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "83/83 [==============================] - 3s 27ms/step\n",
      "11-20 09:23:01 I deeptables.m.deeptable.py 559 - predict_proba taken 3.062842845916748s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 4 | rmse: 0.4537999927997589\n",
      "\n",
      "#########################\n",
      "### Fold 5/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-20 09:23:02 I deeptables.m.deeptable.py 337 - X.Shape=(186516, 596), y.Shape=(186516,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x32460a6b0>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-20 09:23:02 I deeptables.m.deeptable.py 338 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-20 09:23:03 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/mavillan/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-E8z0EJ47-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-20 09:23:03 I deeptables.m.preprocessor.py 261 - Preparing features...\n",
      "11-20 09:23:03 I deeptables.m.preprocessor.py 336 - Preparing features taken 0.267653226852417s\n",
      "11-20 09:23:03 I deeptables.m.preprocessor.py 388 - Categorical encoding...\n",
      "11-20 09:23:03 I deeptables.m.preprocessor.py 393 - Categorical encoding taken 0.12052607536315918s\n",
      "11-20 09:23:03 I deeptables.m.preprocessor.py 423 - Extracting GBM features...\n",
      "11-20 09:23:04 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16880\n",
      "[LightGBM] [Info] Number of data points in the train set: 186516, number of used features: 592\n",
      "[LightGBM] [Info] Start training from score 0.042994\n",
      "11-20 09:23:07 I deeptables.m.preprocessor.py 434 - Extracting gbm features taken 3.7933349609375s\n",
      "11-20 09:23:08 I deeptables.m.preprocessor.py 196 - fit_transform taken 4.852240800857544s\n",
      "11-20 09:23:08 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:23:08 I deeptables.m.preprocessor.py 249 - transform_X taken 0.27974724769592285s\n",
      "11-20 09:23:08 I deeptables.m.preprocessor.py 230 - Transform [y]...\n",
      "11-20 09:23:08 I deeptables.m.preprocessor.py 236 - transform_y taken 5.1975250244140625e-05s\n",
      "11-20 09:23:08 I deeptables.m.deeptable.py 353 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-20 09:23:08 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:23:09 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-20 09:23:09 I deeptables.m.deepmodel.py 231 - Building model...\n",
      "11-20 09:23:10 I deeptables.m.deepmodel.py 287 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (584)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1016)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1016), output_shape (None, 128)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-20 09:23:10 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 86s - loss: 0.1095 - root_mean_squared_error: 0.3309 - val_loss: 0.1821 - val_root_mean_squared_error: 0.4268 - lr: 1.0000e-04 - 86s/epoch - 59ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 78s - loss: 0.0882 - root_mean_squared_error: 0.2970 - val_loss: 0.1784 - val_root_mean_squared_error: 0.4224 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 77s - loss: 0.0698 - root_mean_squared_error: 0.2642 - val_loss: 0.1791 - val_root_mean_squared_error: 0.4232 - lr: 0.0010 - 77s/epoch - 53ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 77s - loss: 0.0605 - root_mean_squared_error: 0.2460 - val_loss: 0.1803 - val_root_mean_squared_error: 0.4246 - lr: 0.0010 - 77s/epoch - 53ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 76s - loss: 0.0536 - root_mean_squared_error: 0.2316 - val_loss: 0.1800 - val_root_mean_squared_error: 0.4243 - lr: 0.0010 - 76s/epoch - 52ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 76s - loss: 0.0479 - root_mean_squared_error: 0.2188 - val_loss: 0.1830 - val_root_mean_squared_error: 0.4278 - lr: 0.0010 - 76s/epoch - 52ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1457/1457 - 78s - loss: 0.0433 - root_mean_squared_error: 0.2081 - val_loss: 0.1802 - val_root_mean_squared_error: 0.4245 - lr: 0.0010 - 78s/epoch - 53ms/step\n",
      "11-20 09:32:18 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-20 09:32:18 I deeptables.m.deeptable.py 369 - Training finished.\n",
      "11-20 09:32:19 I deeptables.m.deeptable.py 704 - Model has been saved to:dt_output/dt_20241120092301_cin_nets_dnn_nets/cin_nets+dnn_nets.h5\n",
      "11-20 09:32:19 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-20 09:32:19 I deeptables.m.preprocessor.py 242 - Transform [X]...\n",
      "11-20 09:32:20 I deeptables.m.preprocessor.py 249 - transform_X taken 0.3043828010559082s\n",
      "11-20 09:32:20 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-20 09:32:20 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "92/92 [==============================] - 3s 24ms/step\n",
      "11-20 09:32:22 I deeptables.m.deeptable.py 559 - predict_proba taken 3.0274767875671387s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 5 | rmse: 0.4244000017642975\n",
      "\n",
      "\u001b[34m\u001b[1mOverall CV rmse: 0.5148\n",
      "\n",
      "\u001b[34m\u001b[1mOOF scores: [0.4343, 0.4611, 0.4574, 0.4538, 0.4244] (avg: 0.4462)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_models = train(train_df, cat_cols, num_cols, scaler) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a68925",
   "metadata": {
    "papermill": {
     "duration": 0.011625,
     "end_time": "2024-11-11T22:56:16.887054",
     "exception": false,
     "start_time": "2024-11-11T22:56:16.875429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-strength-variants-kSTIVMm8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2797.065202,
   "end_time": "2024-11-11T22:56:19.920724",
   "environment_variables": {},
   "exception": null,
   "input_path": "train/dt-custom-exp2.ipynb",
   "output_path": "train/dt-custom-exp2.ipynb",
   "parameters": {},
   "start_time": "2024-11-11T22:09:42.855522",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
