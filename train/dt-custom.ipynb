{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/r3/nfdbt6x17c9fykl9xgm_g9yj0fnsng/T/ipykernel_61614/3131985156.py:22: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "TensorFlow version: 2.15.0, GPU = True\n",
      "DeepTables version: 0.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/dask/dataframe/__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "2024-11-11 11:46:29.437624: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-11-11 11:46:29.437643: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-11-11 11:46:29.437647: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-11-11 11:46:29.437673: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-11 11:46:29.437686: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd, polars as pl\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from colorama import Fore, Style\n",
    "\n",
    "import tensorflow as tf, deeptables as dt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from deeptables.models import DeepTable, ModelConfig\n",
    "from deeptables.models import deepnets\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print('TensorFlow version:',tf.__version__+',',\n",
    "      'GPU =',tf.test.is_gpu_available())\n",
    "print('DeepTables version:',dt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.001 to 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/cdeotte/tensorflow-transformer-0-790/notebook\n",
    "LR_START = 1e-4\n",
    "LR_MAX = 1e-3\n",
    "LR_MIN = 1e-3\n",
    "LR_RAMPUP_EPOCHS = 1\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "EPOCHS = 7\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n",
    "        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n",
    "        phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "        cosine_decay = 0.5 * (1 + math.cos(phase))\n",
    "        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN    \n",
    "    return lr\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "lr_y = [lrfn(x) for x in rng]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(rng, lr_y, '-o')\n",
    "plt.xlabel('Epoch'); plt.ylabel('LR')\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n",
    "      format(lr_y[0], max(lr_y), lr_y[-1]))\n",
    "LR_Scheduler = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_path = '../data/raw/train.csv'\n",
    "    split_agent_features = True\n",
    "    scaler = MinMaxScaler()  # Scaler or None\n",
    "    \n",
    "    nn = True    \n",
    "    folds = 5\n",
    "    epochs = 7\n",
    "    batch_size = 128\n",
    "    LR_Scheduler = [LR_Scheduler]\n",
    "    optimizer = Adam(learning_rate=1e-3)\n",
    "    conf = ModelConfig(\n",
    "        auto_imputation=False,\n",
    "        auto_discrete=False,\n",
    "        auto_discard_unique=True,\n",
    "        categorical_columns='auto',\n",
    "        apply_gbm_features=True,\n",
    "        fixed_embedding_dim=True,\n",
    "        embeddings_output_dim=4,\n",
    "        embedding_dropout=0.2,\n",
    "        nets=['dnn_nets'] + ['fm_nets'] + ['cin_nets'],\n",
    "        dnn_params={\n",
    "            'hidden_units': ((1024, 0.0, True),\n",
    "                             (512, 0.0, True),\n",
    "                             (256, 0.0, True),\n",
    "                             (128, 0.0, True)),\n",
    "            'dnn_activation': 'relu',\n",
    "        },\n",
    "        stacking_op='concat',\n",
    "        output_use_bias=False,\n",
    "        optimizer=optimizer,\n",
    "        task='regression',\n",
    "        loss='auto',\n",
    "        metrics=[\"RootMeanSquaredError\"],\n",
    "        earlystopping_patience=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_cols = pd.read_csv('../data/constant_columns.csv').columns.to_list()\n",
    "target_col = 'utility_agent1'\n",
    "game_col = 'GameRulesetName'\n",
    "game_rule_cols = ['EnglishRules', 'LudRules']\n",
    "output_cols = ['num_wins_agent1', 'num_draws_agent1', 'num_losses_agent1']\n",
    "dropped_cols = ['Id'] + constant_cols + game_rule_cols + output_cols\n",
    "agent_cols = ['agent1', 'agent2']\n",
    "\n",
    "def preprocess_data(df): \n",
    "    df = df.drop(filter(lambda x: x in df.columns, dropped_cols))\n",
    "    if CFG.split_agent_features:\n",
    "        for col in agent_cols:\n",
    "            df = df.with_columns(pl.col(col).str.split(by=\"-\").list.to_struct(fields=lambda idx: f\"{col}_{idx}\")).unnest(col).drop(f\"{col}_0\")\n",
    "    df = df.with_columns([pl.col(col).cast(pl.Categorical) for col in df.columns if col[:6] in agent_cols])            \n",
    "    df = df.with_columns([pl.col(col).cast(pl.Float32) for col in df.columns if col[:6] not in agent_cols and col != game_col])\n",
    "    df = df.to_pandas()\n",
    "    df[\"utility_agent1_rank\"] = (\n",
    "        df[\"utility_agent1\"].rank(method='dense', ascending=True).astype(int)\n",
    "    )\n",
    "    print(f'Data shape: {df.shape}\\n')\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['category']).columns.tolist()\n",
    "    non_cat_cols = df.select_dtypes(exclude=['category']).columns.tolist()\n",
    "    num_cols = [num for num in non_cat_cols if num not in [target_col, game_col, \"utility_agent1_rank\"]]\n",
    "    return df, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "def train(data, cat_cols, num_cols, scaler):\n",
    "    group_col = \"GameRulesetName\"\n",
    "    y_col = \"utility_agent1_rank\"\n",
    "    gkf = StratifiedGroupKFold(n_splits=CFG.folds, random_state=2112, shuffle=True)\n",
    "    split_list = list(gkf.split(data, groups=data[group_col], y=data[y_col]))\n",
    "\n",
    "    X = data.drop([target_col, \"utility_agent1_rank\", game_col], axis=1)\n",
    "    y = data[target_col]\n",
    "    oof = np.zeros(len(data))\n",
    "    nn_models = []\n",
    "    \n",
    "    print('nn = '+str(CFG.nn),'\\n')\n",
    "    \n",
    "    for fi, (train_idx, valid_idx) in enumerate(split_list):\n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Fold {fi+1}/{CFG.folds} ...\")\n",
    "        print(\"#\"*25)\n",
    "        \n",
    "        os.makedirs(f\"../data/working/nn_models/fold{fi}\", exist_ok=True)\n",
    "\n",
    "        if CFG.nn == True:\n",
    "            print('\\n',\"nn model training.\",'\\n')\n",
    "            K.clear_session()\n",
    "            nn_model = DeepTable(config=CFG.conf)\n",
    "            nn_model.fit(X.iloc[train_idx], y.iloc[train_idx],\n",
    "                      validation_data=(X.iloc[valid_idx], y.iloc[valid_idx]),\n",
    "                      callbacks=CFG.LR_Scheduler,\n",
    "                      batch_size=CFG.batch_size, epochs=CFG.epochs, verbose=2)\n",
    "            nn_models.append(nn_model)\n",
    "            \n",
    "            # Save model\n",
    "            # nn_model.save(f'../data/working/nn_models/fold{fi}')\n",
    "        \n",
    "            # Avoid some errors\n",
    "            with K.name_scope(CFG.optimizer.__class__.__name__):\n",
    "                for j, var in enumerate(CFG.optimizer.weights):\n",
    "                    name = 'variable{}'.format(j)\n",
    "                    CFG.optimizer.weights[j] = tf.Variable(var, name=name)\n",
    "            CFG.conf = CFG.conf._replace(optimizer=CFG.optimizer)\n",
    "\n",
    "            oof_preds = nn_model.predict(X.iloc[valid_idx], verbose=1, batch_size=512).flatten()\n",
    "            rmse = np.round(np.sqrt(np.mean((oof_preds - y.iloc[valid_idx])**2)),4)\n",
    "            print(f'{Fore.GREEN}{Style.BRIGHT}\\nFold {fi+1} | rmse: {rmse}\\n')\n",
    "            if fi<CFG.folds: oof[valid_idx] = oof_preds\n",
    "            else: oof[valid_idx] += oof_preds\n",
    "        else:\n",
    "            raise ValueError(\"Neural network model not enabled in CFG.\")\n",
    "    \n",
    "    rmse = np.round(np.sqrt(np.mean((oof - y)**2)),4)\n",
    "    print(f'{Fore.BLUE}{Style.BRIGHT}Overall CV rmse: {rmse}\\n')\n",
    "    plot_model(nn_model.get_model().model)\n",
    "    return nn_models\n",
    "\n",
    "\n",
    "def infer(data, nn_models, ctb_models, num_cols, scaler):\n",
    "    return np.mean(\n",
    "        [model.predict(data, verbose=1, batch_size=512).flatten()\n",
    "         for model in nn_models\n",
    "        ],\n",
    "        axis=0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (233234, 599)\n",
      "\n",
      "Scaling 588 numerical cols.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = CFG.scaler\n",
    "train_df = pl.read_csv(CFG.train_path)\n",
    "train_df, cat_cols, num_cols = preprocess_data(train_df)\n",
    "if scaler is not None:\n",
    "    print(f'Scaling {len(num_cols)} numerical cols.\\n')\n",
    "    train_df[num_cols] = scaler.fit_transform(train_df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn = True \n",
      "\n",
      "#########################\n",
      "### Fold 1/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-11 11:46:31 I deeptables.m.deeptable.py 338 - X.Shape=(186410, 596), y.Shape=(186410,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x35118a650>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-11 11:46:31 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-11 11:46:32 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 11:46:32 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-11 11:46:32 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.25678515434265137s\n",
      "11-11 11:46:32 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-11 11:46:32 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.10229611396789551s\n",
      "11-11 11:46:32 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-11 11:46:33 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16885\n",
      "[LightGBM] [Info] Number of data points in the train set: 186410, number of used features: 584\n",
      "[LightGBM] [Info] Start training from score 0.046962\n",
      "11-11 11:46:36 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 3.541358709335327s\n",
      "11-11 11:46:36 I deeptables.m.preprocessor.py 198 - fit_transform taken 4.453400135040283s\n",
      "11-11 11:46:36 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 11:46:37 I deeptables.m.preprocessor.py 251 - transform_X taken 0.2984731197357178s\n",
      "11-11 11:46:37 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-11 11:46:37 I deeptables.m.preprocessor.py 238 - transform_y taken 4.506111145019531e-05s\n",
      "11-11 11:46:37 I deeptables.m.deeptable.py 354 - Training...\n",
      "11-11 11:46:37 I deeptables.m.deeptable.py 752 - Injected a callback [EarlyStopping]. monitor:val_rootmeansquarederror, patience:1, mode:min\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-11 11:46:37 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:46:37.301471: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-11-11 11:46:37.301487: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 11:46:37 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 11:46:38 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-11 11:46:38 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (576)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1008)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1008), output_shape (None, 128)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-11 11:46:38 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 11:46:42.220451: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 87s - loss: 0.1352 - root_mean_squared_error: 0.3677 - val_loss: 0.2070 - val_root_mean_squared_error: 0.4550 - lr: 1.0000e-04 - 87s/epoch - 60ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 78s - loss: 0.0921 - root_mean_squared_error: 0.3035 - val_loss: 0.1982 - val_root_mean_squared_error: 0.4452 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 78s - loss: 0.0721 - root_mean_squared_error: 0.2686 - val_loss: 0.1962 - val_root_mean_squared_error: 0.4430 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 78s - loss: 0.0625 - root_mean_squared_error: 0.2500 - val_loss: 0.2002 - val_root_mean_squared_error: 0.4474 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 79s - loss: 0.0558 - root_mean_squared_error: 0.2363 - val_loss: 0.1978 - val_root_mean_squared_error: 0.4447 - lr: 0.0010 - 79s/epoch - 54ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 78s - loss: 0.0500 - root_mean_squared_error: 0.2236 - val_loss: 0.1956 - val_root_mean_squared_error: 0.4422 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1456/1456 - 78s - loss: 0.0454 - root_mean_squared_error: 0.2130 - val_loss: 0.1944 - val_root_mean_squared_error: 0.4409 - lr: 0.0010 - 78s/epoch - 54ms/step\n",
      "11-11 11:55:54 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-11 11:55:54 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-11 11:55:55 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-11 11:55:55 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 11:55:55 I deeptables.m.preprocessor.py 251 - transform_X taken 0.332568883895874s\n",
      "11-11 11:55:55 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-11 11:55:55 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "92/92 [==============================] - 3s 23ms/step\n",
      "11-11 11:55:58 I deeptables.m.deeptable.py 559 - predict_proba taken 3.0000152587890625s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 1 | rmse: 0.4406999945640564\n",
      "\n",
      "#########################\n",
      "### Fold 2/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-11 11:55:58 I deeptables.m.deeptable.py 338 - X.Shape=(185920, 596), y.Shape=(185920,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x35118a650>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-11 11:55:58 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-11 11:55:59 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 11:55:59 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-11 11:56:00 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.26376986503601074s\n",
      "11-11 11:56:00 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-11 11:56:00 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.10049581527709961s\n",
      "11-11 11:56:00 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-11 11:56:00 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16903\n",
      "[LightGBM] [Info] Number of data points in the train set: 185920, number of used features: 586\n",
      "[LightGBM] [Info] Start training from score 0.055716\n",
      "11-11 11:56:03 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 3.336329936981201s\n",
      "11-11 11:56:04 I deeptables.m.preprocessor.py 198 - fit_transform taken 4.407490968704224s\n",
      "11-11 11:56:04 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 11:56:04 I deeptables.m.preprocessor.py 251 - transform_X taken 0.26316404342651367s\n",
      "11-11 11:56:04 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-11 11:56:04 I deeptables.m.preprocessor.py 238 - transform_y taken 5.507469177246094e-05s\n",
      "11-11 11:56:04 I deeptables.m.deeptable.py 354 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-11 11:56:04 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 11:56:04 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 11:56:05 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-11 11:56:05 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (578)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1010)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1010), output_shape (None, 128)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-11 11:56:05 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 88s - loss: 0.1499 - root_mean_squared_error: 0.3872 - val_loss: 0.2402 - val_root_mean_squared_error: 0.4901 - lr: 1.0000e-04 - 88s/epoch - 61ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 81s - loss: 0.0974 - root_mean_squared_error: 0.3120 - val_loss: 0.2056 - val_root_mean_squared_error: 0.4535 - lr: 0.0010 - 81s/epoch - 55ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 80s - loss: 0.0738 - root_mean_squared_error: 0.2717 - val_loss: 0.2051 - val_root_mean_squared_error: 0.4529 - lr: 0.0010 - 80s/epoch - 55ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 80s - loss: 0.0651 - root_mean_squared_error: 0.2551 - val_loss: 0.2070 - val_root_mean_squared_error: 0.4550 - lr: 0.0010 - 80s/epoch - 55ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 80s - loss: 0.0584 - root_mean_squared_error: 0.2416 - val_loss: 0.2092 - val_root_mean_squared_error: 0.4574 - lr: 0.0010 - 80s/epoch - 55ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 80s - loss: 0.0522 - root_mean_squared_error: 0.2284 - val_loss: 0.2065 - val_root_mean_squared_error: 0.4545 - lr: 0.0010 - 80s/epoch - 55ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1452/1452 - 81s - loss: 0.0473 - root_mean_squared_error: 0.2175 - val_loss: 0.2060 - val_root_mean_squared_error: 0.4539 - lr: 0.0010 - 81s/epoch - 56ms/step\n",
      "11-11 12:05:34 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-11 12:05:34 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-11 12:05:35 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-11 12:05:35 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:05:35 I deeptables.m.preprocessor.py 251 - transform_X taken 0.3416452407836914s\n",
      "11-11 12:05:35 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-11 12:05:35 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "93/93 [==============================] - 2s 23ms/step\n",
      "11-11 12:05:38 I deeptables.m.deeptable.py 559 - predict_proba taken 2.9758028984069824s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 2 | rmse: 0.4537999927997589\n",
      "\n",
      "#########################\n",
      "### Fold 3/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-11 12:05:39 I deeptables.m.deeptable.py 338 - X.Shape=(187070, 596), y.Shape=(187070,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x35118a650>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-11 12:05:39 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-11 12:05:39 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 12:05:40 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-11 12:05:40 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.2560610771179199s\n",
      "11-11 12:05:40 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-11 12:05:40 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.10245394706726074s\n",
      "11-11 12:05:40 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-11 12:05:40 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 16986\n",
      "[LightGBM] [Info] Number of data points in the train set: 187070, number of used features: 589\n",
      "[LightGBM] [Info] Start training from score 0.050456\n",
      "11-11 12:05:43 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 3.3966610431671143s\n",
      "11-11 12:05:44 I deeptables.m.preprocessor.py 198 - fit_transform taken 4.318215847015381s\n",
      "11-11 12:05:44 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:05:44 I deeptables.m.preprocessor.py 251 - transform_X taken 0.26031064987182617s\n",
      "11-11 12:05:44 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-11 12:05:44 I deeptables.m.preprocessor.py 238 - transform_y taken 0.00013303756713867188s\n",
      "11-11 12:05:44 I deeptables.m.deeptable.py 354 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-11 12:05:44 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:05:45 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:05:45 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-11 12:05:45 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (581)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1013)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1013), output_shape (None, 128)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-11 12:05:45 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 91s - loss: 0.1586 - root_mean_squared_error: 0.3983 - val_loss: 0.2096 - val_root_mean_squared_error: 0.4578 - lr: 1.0000e-04 - 91s/epoch - 62ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 85s - loss: 0.1007 - root_mean_squared_error: 0.3174 - val_loss: 0.1819 - val_root_mean_squared_error: 0.4265 - lr: 0.0010 - 85s/epoch - 58ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 83s - loss: 0.0768 - root_mean_squared_error: 0.2772 - val_loss: 0.1870 - val_root_mean_squared_error: 0.4324 - lr: 0.0010 - 83s/epoch - 57ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 83s - loss: 0.0685 - root_mean_squared_error: 0.2618 - val_loss: 0.1823 - val_root_mean_squared_error: 0.4269 - lr: 0.0010 - 83s/epoch - 57ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 83s - loss: 0.0618 - root_mean_squared_error: 0.2487 - val_loss: 0.1877 - val_root_mean_squared_error: 0.4332 - lr: 0.0010 - 83s/epoch - 57ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 83s - loss: 0.0556 - root_mean_squared_error: 0.2358 - val_loss: 0.1829 - val_root_mean_squared_error: 0.4277 - lr: 0.0010 - 83s/epoch - 57ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1461/1461 - 83s - loss: 0.0503 - root_mean_squared_error: 0.2243 - val_loss: 0.1871 - val_root_mean_squared_error: 0.4325 - lr: 0.0010 - 83s/epoch - 57ms/step\n",
      "11-11 12:15:36 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-11 12:15:36 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-11 12:15:36 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-11 12:15:36 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:15:36 I deeptables.m.preprocessor.py 251 - transform_X taken 0.31528210639953613s\n",
      "11-11 12:15:36 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-11 12:15:36 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "91/91 [==============================] - 3s 24ms/step\n",
      "11-11 12:15:39 I deeptables.m.deeptable.py 559 - predict_proba taken 2.984132766723633s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 3 | rmse: 0.4323999881744385\n",
      "\n",
      "#########################\n",
      "### Fold 4/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-11 12:15:40 I deeptables.m.deeptable.py 338 - X.Shape=(187456, 596), y.Shape=(187456,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x35118a650>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-11 12:15:40 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-11 12:15:41 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 12:15:41 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-11 12:15:41 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.2710230350494385s\n",
      "11-11 12:15:41 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-11 12:15:42 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.10116386413574219s\n",
      "11-11 12:15:42 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-11 12:15:42 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17097\n",
      "[LightGBM] [Info] Number of data points in the train set: 187456, number of used features: 595\n",
      "[LightGBM] [Info] Start training from score 0.033179\n",
      "11-11 12:15:45 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 3.453429937362671s\n",
      "11-11 12:15:45 I deeptables.m.preprocessor.py 198 - fit_transform taken 4.402420997619629s\n",
      "11-11 12:15:45 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:15:46 I deeptables.m.preprocessor.py 251 - transform_X taken 0.4455749988555908s\n",
      "11-11 12:15:46 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-11 12:15:46 I deeptables.m.preprocessor.py 238 - transform_y taken 0.00010919570922851562s\n",
      "11-11 12:15:46 I deeptables.m.deeptable.py 354 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-11 12:15:46 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:15:47 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:15:47 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-11 12:15:47 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (588)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1020)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1020), output_shape (None, 128)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-11 12:15:47 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 94s - loss: 0.1404 - root_mean_squared_error: 0.3746 - val_loss: 0.2146 - val_root_mean_squared_error: 0.4633 - lr: 1.0000e-04 - 94s/epoch - 64ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 87s - loss: 0.0960 - root_mean_squared_error: 0.3098 - val_loss: 0.1989 - val_root_mean_squared_error: 0.4460 - lr: 0.0010 - 87s/epoch - 59ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 87s - loss: 0.0763 - root_mean_squared_error: 0.2763 - val_loss: 0.1907 - val_root_mean_squared_error: 0.4367 - lr: 0.0010 - 87s/epoch - 59ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 87s - loss: 0.0685 - root_mean_squared_error: 0.2617 - val_loss: 0.1927 - val_root_mean_squared_error: 0.4390 - lr: 0.0010 - 87s/epoch - 59ms/step\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 5/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 87s - loss: 0.0623 - root_mean_squared_error: 0.2497 - val_loss: 0.1950 - val_root_mean_squared_error: 0.4416 - lr: 0.0010 - 87s/epoch - 59ms/step\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 6/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 86s - loss: 0.0566 - root_mean_squared_error: 0.2379 - val_loss: 0.1906 - val_root_mean_squared_error: 0.4366 - lr: 0.0010 - 86s/epoch - 59ms/step\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 7/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1464/1464 - 86s - loss: 0.0520 - root_mean_squared_error: 0.2280 - val_loss: 0.1921 - val_root_mean_squared_error: 0.4383 - lr: 0.0010 - 86s/epoch - 59ms/step\n",
      "11-11 12:26:00 I deeptables.m.deepmodel.py 122 - Training finished.\n",
      "11-11 12:26:00 I deeptables.m.deeptable.py 370 - Training finished.\n",
      "11-11 12:26:01 I deeptables.m.deeptable.py 685 - Perform prediction...\n",
      "11-11 12:26:01 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:26:01 I deeptables.m.preprocessor.py 251 - transform_X taken 0.3421361446380615s\n",
      "11-11 12:26:01 I deeptables.m.deepmodel.py 130 - Performing predictions...\n",
      "11-11 12:26:01 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=512, shuffle=False, drop_remainder=False\n",
      "90/90 [==============================] - 2s 23ms/step\n",
      "11-11 12:26:04 I deeptables.m.deeptable.py 559 - predict_proba taken 3.0128190517425537s\n",
      "\u001b[32m\u001b[1m\n",
      "Fold 4 | rmse: 0.4383000135421753\n",
      "\n",
      "#########################\n",
      "### Fold 5/5 ...\n",
      "#########################\n",
      "\n",
      " nn model training. \n",
      "\n",
      "11-11 12:26:05 I deeptables.m.deeptable.py 338 - X.Shape=(186080, 596), y.Shape=(186080,), batch_size=128, config=ModelConfig(name='conf-1', nets=['cin_nets', 'dnn_nets', 'fm_nets'], categorical_columns='auto', exclude_columns=[], task='regression', pos_label=None, metrics=['RootMeanSquaredError'], auto_categorize=False, cat_exponent=0.5, cat_remain_numeric=True, auto_encode_label=True, auto_imputation=False, auto_scale=False, auto_discrete=False, auto_discard_unique=True, apply_gbm_features=True, gbm_params={}, gbm_feature_type='embedding', fixed_embedding_dim=True, embeddings_output_dim=4, embeddings_initializer='uniform', embeddings_regularizer=None, embeddings_activity_regularizer=None, dense_dropout=0, embedding_dropout=0.2, stacking_op='concat', output_use_bias=False, apply_class_weight=False, optimizer=<keras.src.optimizers.legacy.adam.Adam object at 0x35118a650>, loss='auto', dnn_params={'hidden_units': ((1024, 0.0, True), (512, 0.0, True), (256, 0.0, True), (128, 0.0, True)), 'dnn_activation': 'relu'}, autoint_params={'num_attention': 3, 'num_heads': 1, 'dropout_rate': 0, 'use_residual': True}, fgcnn_params={'fg_filters': (14, 16), 'fg_heights': (7, 7), 'fg_pool_heights': (2, 2), 'fg_new_feat_filters': (2, 2)}, fibinet_params={'senet_pooling_op': 'mean', 'senet_reduction_ratio': 3, 'bilinear_type': 'field_interaction'}, cross_params={'num_cross_layer': 4}, pnn_params={'outer_product_kernel_type': 'mat'}, afm_params={'attention_factor': 4, 'dropout_rate': 0}, cin_params={'cross_layer_size': (128, 128), 'activation': 'relu', 'use_residual': False, 'use_bias': False, 'direct': False, 'reduce_D': False}, home_dir=None, monitor_metric=None, earlystopping_patience=1, earlystopping_mode='auto', gpu_usage_strategy='memory_growth', distribute_strategy=None, var_len_categorical_columns=None)\n",
      "11-11 12:26:05 I deeptables.m.deeptable.py 339 - metrics:['RootMeanSquaredError']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-11 12:26:06 W hypernets.t.cache.py 210 - AttributeError: Can't pickle local object 'make_gradient_clipnorm_fn.<locals>.<lambda>'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/cache.py\", line 165, in _cache_call\n",
      "    cache_key = tb.data_hasher()(key_items)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 20, in __call__\n",
      "    for x in self._iter_data(data):\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 58, in _iter_data\n",
      "    yield from self._iter_data(v)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 53, in _iter_data\n",
      "    yield from self._iter_data(x)\n",
      "  File \"/Users/59008/Library/Caches/pypoetry/virtualenvs/mcts-strength-variants-kSTIVMm8-py3.10/lib/python3.10/site-packages/hypernets/tabular/data_hasher.py\", line 61, in _iter_data\n",
      "    pickle.dump(data, buf, protocol=pickle.HIGHEST_PROTOCOL)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-11 12:26:06 I deeptables.m.preprocessor.py 263 - Preparing features...\n",
      "11-11 12:26:07 I deeptables.m.preprocessor.py 338 - Preparing features taken 0.31320691108703613s\n",
      "11-11 12:26:07 I deeptables.m.preprocessor.py 390 - Categorical encoding...\n",
      "11-11 12:26:07 I deeptables.m.preprocessor.py 395 - Categorical encoding taken 0.10121488571166992s\n",
      "11-11 12:26:07 I deeptables.m.preprocessor.py 436 - Extracting GBM features...\n",
      "11-11 12:26:07 I hypernets.t.sklearn_ex.py 640 - LightGBM task:regression\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17043\n",
      "[LightGBM] [Info] Number of data points in the train set: 186080, number of used features: 592\n",
      "[LightGBM] [Info] Start training from score 0.035732\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 447 - Extracting gbm features taken 3.7958481311798096s\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 198 - fit_transform taken 4.808567047119141s\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 244 - Transform [X]...\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 251 - transform_X taken 0.2717881202697754s\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 232 - Transform [y]...\n",
      "11-11 12:26:11 I deeptables.m.preprocessor.py 238 - transform_y taken 0.000125885009765625s\n",
      "11-11 12:26:11 I deeptables.m.deeptable.py 354 - Training...\n",
      "1 Physical GPUs, 1 Logical GPUs\n",
      "11-11 12:26:11 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:26:12 I deeptables.u.dataset_generator.py 250 - create dataset generator with _TFDGForPandas, batch_size=128, shuffle=True, drop_remainder=True\n",
      "11-11 12:26:12 I deeptables.m.deepmodel.py 235 - Building model...\n",
      "11-11 12:26:13 I deeptables.m.deepmodel.py 291 - >>>>>>>>>>>>>>>>>>>>>> Model Desc <<<<<<<<<<<<<<<<<<<<<<< \n",
      "---------------------------------------------------------\n",
      "inputs:\n",
      "---------------------------------------------------------\n",
      "['all_categorical_vars: (108)', 'input_continuous_all: (584)']\n",
      "---------------------------------------------------------\n",
      "embeddings:\n",
      "---------------------------------------------------------\n",
      "input_dims: [6, 5, 5, 4, 6, 5, 5, 4, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31]\n",
      "output_dims: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "dropout: 0.2\n",
      "---------------------------------------------------------\n",
      "dense: dropout: 0\n",
      "batch_normalization: False\n",
      "---------------------------------------------------------\n",
      "concat_embed_dense: shape: (None, 1016)\n",
      "---------------------------------------------------------\n",
      "nets: ['cin_nets', 'dnn_nets', 'fm_nets']\n",
      "---------------------------------------------------------\n",
      "cin: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "dnn: input_shape (None, 1016), output_shape (None, 128)\n",
      "fm: input_shape (None, 108, 4), output_shape (None, 1)\n",
      "---------------------------------------------------------\n",
      "stacking_op: concat\n",
      "---------------------------------------------------------\n",
      "output: activation: None, output_shape: (None, 1), use_bias: False\n",
      "loss: mse\n",
      "optimizer: Adam\n",
      "---------------------------------------------------------\n",
      "\n",
      "11-11 12:26:13 I deeptables.m.deepmodel.py 105 - training...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1453/1453 - 98s - loss: 0.1107 - root_mean_squared_error: 0.3328 - val_loss: 0.2089 - val_root_mean_squared_error: 0.4571 - lr: 1.0000e-04 - 98s/epoch - 68ms/step\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 2/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1453/1453 - 90s - loss: 0.0888 - root_mean_squared_error: 0.2980 - val_loss: 0.2003 - val_root_mean_squared_error: 0.4476 - lr: 0.0010 - 90s/epoch - 62ms/step\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 3/7\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_rootmeansquarederror` which is not available. Available metrics are: loss,root_mean_squared_error,val_loss,val_root_mean_squared_error,lr\n",
      "1453/1453 - 90s - loss: 0.0705 - root_mean_squared_error: 0.2656 - val_loss: 0.2018 - val_root_mean_squared_error: 0.4492 - lr: 0.0010 - 90s/epoch - 62ms/step\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 4/7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn_models = train(train_df, cat_cols, num_cols, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-strength-variants-kSTIVMm8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
