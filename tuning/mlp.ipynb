{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple, List\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_edf\n",
    "    , plot_optimization_history\n",
    "    , plot_parallel_coordinate\n",
    "    , plot_param_importances\n",
    "\n",
    "    , plot_slice\n",
    ")\n",
    "\n",
    "# local modules\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from preproc import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import seed_everything\n",
    "seed_everything(2112, workers=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some paths\n",
    "path_raw = Path(\"../data/raw\")\n",
    "path_processed = Path(\"../data/processed\")\n",
    "path_results = Path(\"../data/results\")\n",
    "\n",
    "# load data\n",
    "df_train = pd.read_csv(path_raw / \"train.csv\")\n",
    "df_test = pd.read_csv(path_raw / \"test.csv\")\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "df_train, df_test, numerical_cols, categorical_cols = preprocess_data(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    scale_utility=True\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Numerical Columns:\", len(numerical_cols))\n",
    "print(\"Categorical Columns:\", len(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_train['utility_agent1_scaled'], bins=100)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Utility Agent 1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[numerical_cols] = df_train[numerical_cols].astype(np.float32)\n",
    "df_train[categorical_cols] = df_train[categorical_cols].astype(np.int32)\n",
    "\n",
    "cat_input_dims = df_train[categorical_cols].nunique(axis=0).values.tolist()\n",
    "print(cat_input_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns of df_train\n",
    "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
    "\n",
    "# Print a message to confirm the scaling\n",
    "print(\"Numerical columns have been scaled using StandardScaler.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "            num_input_dim: int,\n",
    "            cat_input_dims: list[int],\n",
    "            output_dim: int,\n",
    "            layers: str,\n",
    "            dropout: float,\n",
    "            learning_rate: float = 1e-3,\n",
    "            weight_decay: float = 1e-5,\n",
    "            initialization: str = 'kaiming_uniform',\n",
    "            embedding_dim: Optional[List[int]] = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize embedding dimensions if not provided\n",
    "        if embedding_dim is None:\n",
    "            # Rule of thumb: min(50, num_unique // 2 + 1) for each categorical feature\n",
    "            embedding_dim = [min(50, int(1 + np.ceil(np.sqrt(dim)))) for dim in cat_input_dims]\n",
    "\n",
    "        elif len(embedding_dim) != len(cat_input_dims):\n",
    "            raise ValueError(\"Length of embedding_dim must match number of categorical features.\")\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Create embedding layers\n",
    "        self.create_embeddings(cat_input_dims, embedding_dim)\n",
    "\n",
    "        # Create backbone layers\n",
    "        self.create_backbone(num_input_dim, layers)\n",
    "\n",
    "        # Create head layers\n",
    "        self.create_head(output_dim)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.initialization = initialization\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # Initialize lists to store validation outputs\n",
    "        self.validation_targets = []\n",
    "        self.validation_predictions = []\n",
    "\n",
    "    def create_embeddings(self, cat_input_dims: list[int], embedding_dim: list[int]):\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(dim, emb_dim) for dim, emb_dim in zip(cat_input_dims, embedding_dim)]\n",
    "        )\n",
    "\n",
    "    def create_backbone(self, num_input_dim: int, layers: str):\n",
    "        # Calculate total input dimension after embeddings\n",
    "        total_embedding_dim = sum(self.embedding_dim)\n",
    "        total_input_dim = num_input_dim + total_embedding_dim\n",
    "\n",
    "        # Parse layers string\n",
    "        layer_sizes = [int(size) for size in layers.split('-')]\n",
    "\n",
    "        # Create backbone network layers\n",
    "        backbone_layers = []\n",
    "        prev_size = total_input_dim\n",
    "        for size in layer_sizes:\n",
    "            backbone_layers.extend([\n",
    "                nn.BatchNorm1d(prev_size),\n",
    "                nn.Linear(prev_size, size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(self.hparams.dropout),\n",
    "            ])\n",
    "            prev_size = size\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "        self.backbone_output_size = prev_size\n",
    "\n",
    "    def create_head(self, output_dim: int):\n",
    "        # Output layer\n",
    "        self.head = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.backbone_output_size),\n",
    "            nn.Linear(self.backbone_output_size, output_dim)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if any(module is m for m in self.head.modules()):\n",
    "                    nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "                else:\n",
    "                    if self.initialization == 'kaiming_uniform':\n",
    "                        nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "                    elif self.initialization == 'kaiming_normal':\n",
    "                        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                    elif self.initialization == 'xavier_uniform':\n",
    "                        nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                    elif self.initialization == 'xavier_normal':\n",
    "                        nn.init.xavier_normal_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported initialization method: {self.initialization}\")\n",
    "                \n",
    "                # Initialize bias to small values\n",
    "                if module.bias is not None:\n",
    "                    nn.init.uniform_(module.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        # Process categorical variables\n",
    "        embedded = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        embedded = torch.cat(embedded, dim=1)\n",
    "        \n",
    "        # Concatenate numerical and embedded categorical features\n",
    "        x = torch.cat([x_num, embedded], dim=1)\n",
    "        \n",
    "        # Pass through backbone\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Pass through head\n",
    "        x = self.head(x)\n",
    "        x = nn.functional.hardtanh(x)\n",
    "\n",
    "        return x.squeeze(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('valid_loss', loss, prog_bar=True)\n",
    "        # Store targets and predictions for later use\n",
    "        self.validation_targets.append(y)\n",
    "        self.validation_predictions.append(y_hat)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        if len(batch) == 2:\n",
    "            x_num, x_cat = batch\n",
    "        elif len(batch) == 3:\n",
    "            x_num, x_cat, _ = batch\n",
    "        y_hat = self(x_num, x_cat)\n",
    "        return y_hat\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all targets and predictions\n",
    "        y = torch.cat(self.validation_targets)\n",
    "        y_hat = torch.cat(self.validation_predictions)\n",
    "        rmse = torch.sqrt(F.mse_loss(y_hat, y))\n",
    "        self.log('val_rmse', rmse, prog_bar=True)\n",
    "        # Clear the lists for next epoch\n",
    "        self.validation_targets.clear()\n",
    "        self.validation_predictions.clear()\n",
    "                \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.learning_rate, \n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.learning_rate,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.2,\n",
    "            anneal_strategy='cos',\n",
    "            cycle_momentum=True,\n",
    "            base_momentum=0.85,\n",
    "            max_momentum=0.95,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "# Define the column for stratified or group k-fold\n",
    "groups_col = 'GameRulesetName'\n",
    "gkf = GroupKFold(n_splits=num_folds)\n",
    "split_list = list(gkf.split(df_train, groups=df_train[groups_col]))\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_rmse',\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def train_and_score(\n",
    "        batch_size,\n",
    "        dropout,\n",
    "        input_layer_size,\n",
    "        n_layers,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        initialization,\n",
    "        pct_start,\n",
    "    ):\n",
    "    oof_scores = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for _, (train_index, val_index) in enumerate(split_list, 1):\n",
    "        \n",
    "        # Split the data\n",
    "        train, valid = df_train.iloc[train_index], df_train.iloc[val_index]\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(train[numerical_cols].values, dtype=torch.float32),\n",
    "            torch.tensor(train[categorical_cols].values, dtype=torch.int32),\n",
    "            torch.tensor(train['utility_agent1'].values, dtype=torch.float32)\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        valid_dataset = TensorDataset(\n",
    "            torch.tensor(valid[numerical_cols].values, dtype=torch.float32),\n",
    "            torch.tensor(valid[categorical_cols].values, dtype=torch.int32),\n",
    "            torch.tensor(valid['utility_agent1'].values, dtype=torch.float32)\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        # Construct the layers string based on input_layer_size and n_layers\n",
    "        layers = \"-\".join([str(input_layer_size // (2 ** i)) for i in range(n_layers)])\n",
    "\n",
    "        model = MLP(\n",
    "            num_input_dim=len(numerical_cols),\n",
    "            cat_input_dims=cat_input_dims,\n",
    "            output_dim=1,\n",
    "            layers=layers,\n",
    "            dropout=dropout,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            initialization=initialization,\n",
    "            pct_start=pct_start,\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            min_epochs=20,\n",
    "            max_epochs=100, \n",
    "            deterministic=True,\n",
    "            accelerator=\"mps\",\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "            callbacks=[\n",
    "                early_stop_callback, \n",
    "                ModelCheckpoint(monitor='val_rmse', mode='min', save_top_k=1),\n",
    "            ],\n",
    "        );\n",
    "        trainer.fit(\n",
    "            model, \n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "        );\n",
    "\n",
    "        # Load the best model\n",
    "        best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "        model = MLP.load_from_checkpoint(best_model_path)\n",
    "\n",
    "        # Predict on validation set using trainer.predict with the prediction DataLoader\n",
    "        predictions = trainer.predict(model, dataloaders=valid_loader)\n",
    "        y_pred = torch.cat(predictions).squeeze().cpu().numpy()\n",
    "         \n",
    "        # Compute RMSE on scaled values\n",
    "        y_valid = valid['utility_agent1'].values\n",
    "        rmse = np.sqrt(np.mean((y_pred - y_valid) ** 2))\n",
    "        oof_scores.append(rmse)\n",
    "\n",
    "    return np.mean(oof_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Suggest hyperparameters to tune\n",
    "    batch_size = trial.suggest_int('batch_size', 128, 512, step=64)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.3, step=0.05)\n",
    "    input_layer_size = trial.suggest_int('input_layer_size', 512, 1024, step=64)\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)\n",
    "    pct_start = trial.suggest_float('pct_start', 0.05, 0.2, step=0.05)\n",
    "    div_factor = trial.suggest_float('div_factor', 1.0, 100.0, step=1.0)\n",
    "    final_div_factor = trial.suggest_float('final_div_factor', 1e0, 1e4, log=True)\n",
    "\n",
    "    # Train and evaluate the model with the suggested hyperparameters\n",
    "    oof_score = train_and_score(\n",
    "        batch_size=batch_size,\n",
    "        dropout=dropout,\n",
    "        input_layer_size=input_layer_size,\n",
    "        n_layers=n_layers,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        initialization=\"kaiming_uniform\",\n",
    "        pct_start=pct_start,\n",
    "        div_factor=div_factor,\n",
    "        final_div_factor=final_div_factor,\n",
    "    )\n",
    "\n",
    "    return oof_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_optimize = True\n",
    "timeout = 3600 * 24\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"mlp\",\n",
    "    direction='minimize',\n",
    "    storage='sqlite:///mlp.db',\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "if do_optimize:\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=1000, \n",
    "        timeout=timeout,\n",
    "        n_jobs=1, \n",
    "        gc_after_trial=True,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values(\"value\", ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = dict(study.best_params)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-strength-variants-kSTIVMm8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
